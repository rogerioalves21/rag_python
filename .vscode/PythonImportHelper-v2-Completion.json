[
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncIterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncIterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncIterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "importPath": "app.api.extractors.base_extractor",
        "description": "app.api.extractors.base_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.base_extractor",
        "documentation": {}
    },
    {
        "label": "LocalStorage",
        "importPath": "app.storage.local_storage",
        "description": "app.storage.local_storage",
        "isExtraImport": true,
        "detail": "app.storage.local_storage",
        "documentation": {}
    },
    {
        "label": "LocalStorage",
        "importPath": "app.storage.local_storage",
        "description": "app.storage.local_storage",
        "isExtraImport": true,
        "detail": "app.storage.local_storage",
        "documentation": {}
    },
    {
        "label": "PyPDFium2Loader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "WebBaseLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "UnstructuredExcelLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "UnstructuredWordDocumentLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "isExtraImport": true,
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "wait",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "wait",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "wait",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "wait",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "Documento",
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "PdfToTextExtrator",
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "PdfToTextExtrator",
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "Reader",
        "importPath": "pdfdataextractor",
        "description": "pdfdataextractor",
        "isExtraImport": true,
        "detail": "pdfdataextractor",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Language",
        "importPath": "langchain_text_splitters.base",
        "description": "langchain_text_splitters.base",
        "isExtraImport": true,
        "detail": "langchain_text_splitters.base",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "langchain_text_splitters.base",
        "description": "langchain_text_splitters.base",
        "isExtraImport": true,
        "detail": "langchain_text_splitters.base",
        "documentation": {}
    },
    {
        "label": "TextParser",
        "importPath": "app.api.prepdoclib.textparser",
        "description": "app.api.prepdoclib.textparser",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.textparser",
        "documentation": {}
    },
    {
        "label": "TextParser",
        "importPath": "app.api.prepdoclib.textparser",
        "description": "app.api.prepdoclib.textparser",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.textparser",
        "documentation": {}
    },
    {
        "label": "TextParser",
        "importPath": "app.api.prepdoclib.textparser",
        "description": "app.api.prepdoclib.textparser",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.textparser",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "MatLike",
        "importPath": "cv2.typing",
        "description": "cv2.typing",
        "isExtraImport": true,
        "detail": "cv2.typing",
        "documentation": {}
    },
    {
        "label": "MatLike",
        "importPath": "cv2.typing",
        "description": "cv2.typing",
        "isExtraImport": true,
        "detail": "cv2.typing",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "Page",
        "importPath": "fitz",
        "description": "fitz",
        "isExtraImport": true,
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "pytesseract",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytesseract",
        "description": "pytesseract",
        "detail": "pytesseract",
        "documentation": {}
    },
    {
        "label": "Output",
        "importPath": "pytesseract",
        "description": "pytesseract",
        "isExtraImport": true,
        "detail": "pytesseract",
        "documentation": {}
    },
    {
        "label": "Output",
        "importPath": "pytesseract",
        "description": "pytesseract",
        "isExtraImport": true,
        "detail": "pytesseract",
        "documentation": {}
    },
    {
        "label": "BaseBlobParser",
        "importPath": "langchain_core.document_loaders.base",
        "description": "langchain_core.document_loaders.base",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.base",
        "documentation": {}
    },
    {
        "label": "BaseBlobParser",
        "importPath": "langchain_core.document_loaders.base",
        "description": "langchain_core.document_loaders.base",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.base",
        "documentation": {}
    },
    {
        "label": "BaseBlobParser",
        "importPath": "langchain_core.document_loaders.base",
        "description": "langchain_core.document_loaders.base",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.base",
        "documentation": {}
    },
    {
        "label": "Blob",
        "importPath": "langchain_core.document_loaders.blob_loaders",
        "description": "langchain_core.document_loaders.blob_loaders",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.blob_loaders",
        "documentation": {}
    },
    {
        "label": "Blob",
        "importPath": "langchain_core.document_loaders.blob_loaders",
        "description": "langchain_core.document_loaders.blob_loaders",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.blob_loaders",
        "documentation": {}
    },
    {
        "label": "Blob",
        "importPath": "langchain_core.document_loaders.blob_loaders",
        "description": "langchain_core.document_loaders.blob_loaders",
        "isExtraImport": true,
        "detail": "langchain_core.document_loaders.blob_loaders",
        "documentation": {}
    },
    {
        "label": "BasePDFLoader",
        "importPath": "langchain_community.document_loaders.pdf",
        "description": "langchain_community.document_loaders.pdf",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders.pdf",
        "documentation": {}
    },
    {
        "label": "BasePDFLoader",
        "importPath": "langchain_community.document_loaders.pdf",
        "description": "langchain_community.document_loaders.pdf",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders.pdf",
        "documentation": {}
    },
    {
        "label": "BasePDFLoader",
        "importPath": "langchain_community.document_loaders.pdf",
        "description": "langchain_community.document_loaders.pdf",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders.pdf",
        "documentation": {}
    },
    {
        "label": "CleanSymbolsProcessor",
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "CleanSymbolsProcessor",
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "CleanSymbolsProcessor",
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "show_paragraphs",
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "CleanSymbolsProcessor",
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "BackgroundTasks",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Header",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "InformacoesUsuario",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "RagService",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "RagService",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "InformacoesUsuario",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "RagService",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "RagService",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "InformacoesUsuario",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_text_splitter",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_chat_ollama_client",
        "importPath": "app.config",
        "description": "app.config",
        "isExtraImport": true,
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "CheckDocumentsPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "CheckDocumentsResponse",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationResponse",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationResponse",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "NormativosResponse",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "SourceModel",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "SignedUrls",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "UserControlServicePayload",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "UserControlServiceResponse",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "isExtraImport": true,
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "langchain_text_splitters",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "MongoDBAtlasVectorSearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DuckDB",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "MongoDBAtlasVectorSearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DuckDB",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "DocArrayInMemorySearch",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "AsyncIteratorCallbackHandler",
        "importPath": "langchain.callbacks",
        "description": "langchain.callbacks",
        "isExtraImport": true,
        "detail": "langchain.callbacks",
        "documentation": {}
    },
    {
        "label": "AsyncIteratorCallbackHandler",
        "importPath": "langchain.callbacks",
        "description": "langchain.callbacks",
        "isExtraImport": true,
        "detail": "langchain.callbacks",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ollama",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ollama",
        "description": "ollama",
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "generate",
        "importPath": "ollama",
        "description": "ollama",
        "isExtraImport": true,
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "generate",
        "importPath": "ollama",
        "description": "ollama",
        "isExtraImport": true,
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "ollama",
        "description": "ollama",
        "isExtraImport": true,
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "AsyncClient",
        "importPath": "ollama",
        "description": "ollama",
        "isExtraImport": true,
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "DataAnalysisService",
        "importPath": "app.api.services.data_analysis_service",
        "description": "app.api.services.data_analysis_service",
        "isExtraImport": true,
        "detail": "app.api.services.data_analysis_service",
        "documentation": {}
    },
    {
        "label": "ComunicadoTextSplitter",
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "clean_query",
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "ComunicadoTextSplitter",
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "RetrievalQAWithSourcesChain",
        "importPath": "langchain.chains.qa_with_sources.retrieval",
        "description": "langchain.chains.qa_with_sources.retrieval",
        "isExtraImport": true,
        "detail": "langchain.chains.qa_with_sources.retrieval",
        "documentation": {}
    },
    {
        "label": "LongContextReorder",
        "importPath": "langchain_community.document_transformers",
        "description": "langchain_community.document_transformers",
        "isExtraImport": true,
        "detail": "langchain_community.document_transformers",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains.conversational_retrieval.base",
        "description": "langchain.chains.conversational_retrieval.base",
        "isExtraImport": true,
        "detail": "langchain.chains.conversational_retrieval.base",
        "documentation": {}
    },
    {
        "label": "PyTesseractLoader",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "PyTesseractLoader",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "isExtraImport": true,
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "isExtraImport": true,
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "ParentDocumentRetriever",
        "importPath": "langchain.retrievers",
        "description": "langchain.retrievers",
        "isExtraImport": true,
        "detail": "langchain.retrievers",
        "documentation": {}
    },
    {
        "label": "MergerRetriever",
        "importPath": "langchain.retrievers",
        "description": "langchain.retrievers",
        "isExtraImport": true,
        "detail": "langchain.retrievers",
        "documentation": {}
    },
    {
        "label": "ContextualCompressionRetriever",
        "importPath": "langchain.retrievers",
        "description": "langchain.retrievers",
        "isExtraImport": true,
        "detail": "langchain.retrievers",
        "documentation": {}
    },
    {
        "label": "InMemoryStore",
        "importPath": "langchain.storage",
        "description": "langchain.storage",
        "isExtraImport": true,
        "detail": "langchain.storage",
        "documentation": {}
    },
    {
        "label": "InMemoryStore",
        "importPath": "langchain.storage",
        "description": "langchain.storage",
        "isExtraImport": true,
        "detail": "langchain.storage",
        "documentation": {}
    },
    {
        "label": "weaviate",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "weaviate",
        "description": "weaviate",
        "detail": "weaviate",
        "documentation": {}
    },
    {
        "label": "WeaviateClient",
        "importPath": "weaviate",
        "description": "weaviate",
        "isExtraImport": true,
        "detail": "weaviate",
        "documentation": {}
    },
    {
        "label": "WeaviateClient",
        "importPath": "weaviate",
        "description": "weaviate",
        "isExtraImport": true,
        "detail": "weaviate",
        "documentation": {}
    },
    {
        "label": "load_qa_chain",
        "importPath": "langchain.chains.question_answering",
        "description": "langchain.chains.question_answering",
        "isExtraImport": true,
        "detail": "langchain.chains.question_answering",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "get_signed_url",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "user_control_service",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "check_documents",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "conversation",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "conversation2",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "parent",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "conversation_normativos",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "data_analysis",
        "importPath": "app.api.routes",
        "description": "app.api.routes",
        "isExtraImport": true,
        "detail": "app.api.routes",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "BaseStorage",
        "importPath": "app.storage.base_storage",
        "description": "app.storage.base_storage",
        "isExtraImport": true,
        "detail": "app.storage.base_storage",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "VectorStore",
        "importPath": "langchain_core.vectorstore",
        "description": "langchain_core.vectorstore",
        "isExtraImport": true,
        "detail": "langchain_core.vectorstore",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "MongoClient",
        "importPath": "pymongo",
        "description": "pymongo",
        "isExtraImport": true,
        "detail": "pymongo",
        "documentation": {}
    },
    {
        "label": "StaticFiles",
        "importPath": "fastapi.staticfiles",
        "description": "fastapi.staticfiles",
        "isExtraImport": true,
        "detail": "fastapi.staticfiles",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "cors_origins",
        "importPath": "app.utils",
        "description": "app.utils",
        "isExtraImport": true,
        "detail": "app.utils",
        "documentation": {}
    },
    {
        "label": "ComunicadoTextSplitter",
        "importPath": "app.utils",
        "description": "app.utils",
        "isExtraImport": true,
        "detail": "app.utils",
        "documentation": {}
    },
    {
        "label": "tratar_linhas_texto",
        "importPath": "app.utils",
        "description": "app.utils",
        "isExtraImport": true,
        "detail": "app.utils",
        "documentation": {}
    },
    {
        "label": "api_router",
        "importPath": "app.api.main",
        "description": "app.api.main",
        "isExtraImport": true,
        "detail": "app.api.main",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "set_debug",
        "importPath": "langchain.globals",
        "description": "langchain.globals",
        "isExtraImport": true,
        "detail": "langchain.globals",
        "documentation": {}
    },
    {
        "label": "set_llm_cache",
        "importPath": "langchain.globals",
        "description": "langchain.globals",
        "isExtraImport": true,
        "detail": "langchain.globals",
        "documentation": {}
    },
    {
        "label": "pymupdf4llm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pymupdf4llm",
        "description": "pymupdf4llm",
        "detail": "pymupdf4llm",
        "documentation": {}
    },
    {
        "label": "pymupdf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pymupdf",
        "description": "pymupdf",
        "detail": "pymupdf",
        "documentation": {}
    },
    {
        "label": "tkinter",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tkinter",
        "description": "tkinter",
        "detail": "tkinter",
        "documentation": {}
    },
    {
        "label": "scrolledtext",
        "importPath": "tkinter",
        "description": "tkinter",
        "isExtraImport": true,
        "detail": "tkinter",
        "documentation": {}
    },
    {
        "label": "ttk",
        "importPath": "tkinter",
        "description": "tkinter",
        "isExtraImport": true,
        "detail": "tkinter",
        "documentation": {}
    },
    {
        "label": "messagebox",
        "importPath": "tkinter",
        "description": "tkinter",
        "isExtraImport": true,
        "detail": "tkinter",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "websockets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "websockets",
        "description": "websockets",
        "detail": "websockets",
        "documentation": {}
    },
    {
        "label": "bs4",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bs4",
        "description": "bs4",
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "create_history_aware_retriever",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "create_retrieval_chain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "create_stuff_documents_chain",
        "importPath": "langchain.chains.combine_documents",
        "description": "langchain.chains.combine_documents",
        "isExtraImport": true,
        "detail": "langchain.chains.combine_documents",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "BaseChatMessageHistory",
        "importPath": "langchain_core.chat_history",
        "description": "langchain_core.chat_history",
        "isExtraImport": true,
        "detail": "langchain_core.chat_history",
        "documentation": {}
    },
    {
        "label": "RunnableWithMessageHistory",
        "importPath": "langchain_core.runnables.history",
        "description": "langchain_core.runnables.history",
        "isExtraImport": true,
        "detail": "langchain_core.runnables.history",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "TSNE",
        "importPath": "sklearn.manifold",
        "description": "sklearn.manifold",
        "isExtraImport": true,
        "detail": "sklearn.manifold",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "fasttext.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fasttext.util",
        "description": "fasttext.util",
        "detail": "fasttext.util",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "LatentDirichletAllocation",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "importPath": "rag_service",
        "description": "rag_service",
        "isExtraImport": true,
        "detail": "rag_service",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "importPath": "rag_service",
        "description": "rag_service",
        "isExtraImport": true,
        "detail": "rag_service",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "importPath": "rag_service",
        "description": "rag_service",
        "isExtraImport": true,
        "detail": "rag_service",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "importPath": "rag_service",
        "description": "rag_service",
        "isExtraImport": true,
        "detail": "rag_service",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "langchain_community",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain_community",
        "description": "langchain_community",
        "detail": "langchain_community",
        "documentation": {}
    },
    {
        "label": "SentenceTransformerEmbeddings",
        "importPath": "langchain_community.embeddings.sentence_transformer",
        "description": "langchain_community.embeddings.sentence_transformer",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.sentence_transformer",
        "documentation": {}
    },
    {
        "label": "SentenceTransformerEmbeddings",
        "importPath": "langchain_community.embeddings.sentence_transformer",
        "description": "langchain_community.embeddings.sentence_transformer",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.sentence_transformer",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain_community.llms.ollama",
        "description": "langchain_community.llms.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.llms.ollama",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain_community.llms.ollama",
        "description": "langchain_community.llms.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.llms.ollama",
        "documentation": {}
    },
    {
        "label": "pandasai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandasai",
        "description": "pandasai",
        "detail": "pandasai",
        "documentation": {}
    },
    {
        "label": "SmartDataframe",
        "importPath": "pandasai",
        "description": "pandasai",
        "isExtraImport": true,
        "detail": "pandasai",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "load_workbook",
        "importPath": "openpyxl",
        "description": "openpyxl",
        "isExtraImport": true,
        "detail": "openpyxl",
        "documentation": {}
    },
    {
        "label": "sys;",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys;",
        "description": "sys;",
        "detail": "sys;",
        "documentation": {}
    },
    {
        "label": "SQLiteCache",
        "importPath": "langchain.cache",
        "description": "langchain.cache",
        "isExtraImport": true,
        "detail": "langchain.cache",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFont",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageDraw",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "convert_from_path",
        "importPath": "pdf2image",
        "description": "pdf2image",
        "isExtraImport": true,
        "detail": "pdf2image",
        "documentation": {}
    },
    {
        "label": "Llama",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "LlamaGrammar",
        "importPath": "llama_cpp",
        "description": "llama_cpp",
        "isExtraImport": true,
        "detail": "llama_cpp",
        "documentation": {}
    },
    {
        "label": "tiktoken",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tiktoken",
        "description": "tiktoken",
        "detail": "tiktoken",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "decouple",
        "description": "decouple",
        "isExtraImport": true,
        "detail": "decouple",
        "documentation": {}
    },
    {
        "label": "RepositoryEnv",
        "importPath": "decouple",
        "description": "decouple",
        "isExtraImport": true,
        "detail": "decouple",
        "documentation": {}
    },
    {
        "label": "FileLock",
        "importPath": "filelock",
        "description": "filelock",
        "isExtraImport": true,
        "detail": "filelock",
        "documentation": {}
    },
    {
        "label": "Timeout",
        "importPath": "filelock",
        "description": "filelock",
        "isExtraImport": true,
        "detail": "filelock",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForObjectDetection",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AsyncOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AsyncAnthropic",
        "importPath": "anthropic",
        "description": "anthropic",
        "isExtraImport": true,
        "detail": "anthropic",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ocrmypdf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ocrmypdf",
        "description": "ocrmypdf",
        "detail": "ocrmypdf",
        "documentation": {}
    },
    {
        "label": "SelfQueryRetriever",
        "importPath": "langchain.retrievers.self_query.base",
        "description": "langchain.retrievers.self_query.base",
        "isExtraImport": true,
        "detail": "langchain.retrievers.self_query.base",
        "documentation": {}
    },
    {
        "label": "AttributeInfo",
        "importPath": "langchain.chains.query_constructor.base",
        "description": "langchain.chains.query_constructor.base",
        "isExtraImport": true,
        "detail": "langchain.chains.query_constructor.base",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "serve",
        "importPath": "ray",
        "description": "ray",
        "isExtraImport": true,
        "detail": "ray",
        "documentation": {}
    },
    {
        "label": "InferenceClient",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "codecs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "codecs",
        "description": "codecs",
        "detail": "codecs",
        "documentation": {}
    },
    {
        "label": "weaviate.classes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "weaviate.classes",
        "description": "weaviate.classes",
        "detail": "weaviate.classes",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "Configure",
        "importPath": "weaviate.classes.config",
        "description": "weaviate.classes.config",
        "isExtraImport": true,
        "detail": "weaviate.classes.config",
        "documentation": {}
    },
    {
        "label": "Configure",
        "importPath": "weaviate.classes.config",
        "description": "weaviate.classes.config",
        "isExtraImport": true,
        "detail": "weaviate.classes.config",
        "documentation": {}
    },
    {
        "label": "EmbeddedOptions",
        "importPath": "weaviate.embedded",
        "description": "weaviate.embedded",
        "isExtraImport": true,
        "detail": "weaviate.embedded",
        "documentation": {}
    },
    {
        "label": "EmbeddedOptions",
        "importPath": "weaviate.embedded",
        "description": "weaviate.embedded",
        "isExtraImport": true,
        "detail": "weaviate.embedded",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "matplotlib.patches",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "Patch",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "LLMChainExtractor",
        "importPath": "langchain.retrievers.document_compressors",
        "description": "langchain.retrievers.document_compressors",
        "isExtraImport": true,
        "detail": "langchain.retrievers.document_compressors",
        "documentation": {}
    },
    {
        "label": "AdditionalConfig",
        "importPath": "weaviate.config",
        "description": "weaviate.config",
        "isExtraImport": true,
        "detail": "weaviate.config",
        "documentation": {}
    },
    {
        "label": "Timeout",
        "importPath": "weaviate.config",
        "description": "weaviate.config",
        "isExtraImport": true,
        "detail": "weaviate.config",
        "documentation": {}
    },
    {
        "label": "MetadataQuery",
        "importPath": "weaviate.classes.query",
        "description": "weaviate.classes.query",
        "isExtraImport": true,
        "detail": "weaviate.classes.query",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "kind": 6,
        "importPath": "app.api.extractors.base_extractor",
        "description": "app.api.extractors.base_extractor",
        "peekOfCode": "class BaseExtractor(ABC):\n    \"\"\"Interface for extract files.\n    \"\"\"\n    @abstractmethod\n    def extract_all(self) -> List[Document]:\n        raise NotImplementedError\n    @abstractmethod\n    def extract(self) -> List[Document]:\n        raise NotImplementedError",
        "detail": "app.api.extractors.base_extractor",
        "documentation": {}
    },
    {
        "label": "PdfExtractor",
        "kind": 6,
        "importPath": "app.api.extractors.pdf_extractor",
        "description": "app.api.extractors.pdf_extractor",
        "peekOfCode": "class PdfExtractor(BaseExtractor):\n    def __init__(\n        self,\n        file_path: str,\n        file_cache_key: Optional[str] = None,\n        metadata_service: Optional[MetadataService] = None\n    ):\n        self._file_path      = file_path\n        self._file_cache_key = file_cache_key.replace('.pdf', '.json')\n        self._storate = LocalStorage()",
        "detail": "app.api.extractors.pdf_extractor",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "app.api.extractors.pdf_outro_extractor",
        "description": "app.api.extractors.pdf_outro_extractor",
        "peekOfCode": "path = r'the path to the PDF file'\n# Create an instance\nfile = Reader()\n# Read the file\npdf = file.read_file(path)\n# Get Caption\npdf.caption()\n# Get Keywords\npdf.keywords()\n# Get Title",
        "detail": "app.api.extractors.pdf_outro_extractor",
        "documentation": {}
    },
    {
        "label": "file",
        "kind": 5,
        "importPath": "app.api.extractors.pdf_outro_extractor",
        "description": "app.api.extractors.pdf_outro_extractor",
        "peekOfCode": "file = Reader()\n# Read the file\npdf = file.read_file(path)\n# Get Caption\npdf.caption()\n# Get Keywords\npdf.keywords()\n# Get Title\npdf.title()\n# Get DOI",
        "detail": "app.api.extractors.pdf_outro_extractor",
        "documentation": {}
    },
    {
        "label": "pdf",
        "kind": 5,
        "importPath": "app.api.extractors.pdf_outro_extractor",
        "description": "app.api.extractors.pdf_outro_extractor",
        "peekOfCode": "pdf = file.read_file(path)\n# Get Caption\npdf.caption()\n# Get Keywords\npdf.keywords()\n# Get Title\npdf.title()\n# Get DOI\npdf.doi()\n# Get Abstract",
        "detail": "app.api.extractors.pdf_outro_extractor",
        "documentation": {}
    },
    {
        "label": "Documento",
        "kind": 6,
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "peekOfCode": "class Documento(BaseModel):\n    \"\"\"Classe para textos e metadatas.\"\"\"\n    conteudo: str\n    metadata: Optional[dict] = Field(default_factory=dict)\nclass AbstractExtrator(ABC):\n    \"\"\"Interface para extrao de textos.\n    \"\"\"\n    @abstractmethod\n    def extrair_texto(self) -> Documento:\n        raise NotImplementedError",
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "AbstractExtrator",
        "kind": 6,
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "peekOfCode": "class AbstractExtrator(ABC):\n    \"\"\"Interface para extrao de textos.\n    \"\"\"\n    @abstractmethod\n    def extrair_texto(self) -> Documento:\n        raise NotImplementedError\nclass PdfToTextExtrator(AbstractExtrator):\n    \"\"\" Extrai texto do PDF no layout mais prximo ao original \"\"\"\n    def extrair_texto(self, _arquivo: str) -> Documento:\n        # if not os.path.isfile(_arquivo): raise RuntimeError(f\"O arquivo informado no existe! -> {_arquivo}\")",
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "PdfToTextExtrator",
        "kind": 6,
        "importPath": "app.api.extractors.pdf_to_text",
        "description": "app.api.extractors.pdf_to_text",
        "peekOfCode": "class PdfToTextExtrator(AbstractExtrator):\n    \"\"\" Extrai texto do PDF no layout mais prximo ao original \"\"\"\n    def extrair_texto(self, _arquivo: str) -> Documento:\n        # if not os.path.isfile(_arquivo): raise RuntimeError(f\"O arquivo informado no existe! -> {_arquivo}\")\n        # _is_windows = os.name == 'nt'\n        # if _is_windows: raise RuntimeError(\"pdftotext no existe no windows!\")\n        if shutil.which('pdftotext'):\n            # _cmd = [\"pdftotext\", \"-layout\", \"-nodiag\", \"-enc\", \"UTF-8\", \"-colspacing\", \"0.3\"]\n            _cmd = [\"pdftotext\", \"-table\", \"-nodiag\", \"-enc\", \"UTF-8\"]\n            _cmd += [_arquivo, \"-\"]",
        "detail": "app.api.extractors.pdf_to_text",
        "documentation": {}
    },
    {
        "label": "Paragraphs",
        "kind": 6,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "class Paragraphs:\n    def __init__(self, __texto: str, separator='\\n'):\n        self.seq = __texto.splitlines()\n        self.line_num = 0    # current index into self.seq (line number)\n        self.para_num = 0    # current index into self (paragraph number)\n        # Ensure that separator string includes a line-end character at the end\n        if separator[-1:] != '\\n': separator += '\\n'\n        self.separator = separator\n    def __getitem__(self, index):\n        if index != self.para_num:",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "CleanSymbolsProcessor",
        "kind": 6,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "class CleanSymbolsProcessor():\n    def __init__(self):\n        # nao faz nada\n        self.inicio = None\n    def process_line(self, line: str) -> Optional[str]:\n        line = line.replace('\"', '')\n        line = line.replace('S \\' CO OB', 'SICOOB')\n        line = line.replace('vSICOOB', 'SICOOB')\n        line = line.replace('Vs  COOB', 'SICOOB')\n        line = line.replace('S T11', 'STI')",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "show_paragraphs",
        "kind": 2,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "def show_paragraphs(text: str, numpars=1000):\n    output = []\n    pp = Paragraphs(text)\n    for p in pp:\n        output.append(repr(p).replace('\\\\n', ' '))\n        if pp.para_num>numpars: break\n    return output\nclass CleanSymbolsProcessor():\n    def __init__(self):\n        # nao faz nada",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "DOUBLE_QUOTES",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "DOUBLE_QUOTES = (\n    '',\n    '',\n    '',\n    # https://www.htmlsymbols.xyz/punctuation-symbols/quotation-mark\n    # '\\u0022' == '\"'\n    '\\u02BA',  # \n    '\\u030B',  # \n    '\\u030E',  # \n    '\\u05F4',  # ",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "SINGLE_QUOTES",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "SINGLE_QUOTES = (\n    # https://github.com/jfilter/clean-text/blob/master/cleantext/constants.py#L115\n    '',\n    '',\n    '',\n    '',\n    '',\n    '`',\n    '',\n)",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "DASHES",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "DASHES = (\n    '',\n    '&',\n    '',  # em dash\n    '',  # en dash\n    '',  # horizontal bar\n)\nSPACES = (\n    # https://www.htmlsymbols.xyz/punctuation-symbols/space-symbols\n    # Run in Dev Browser Console:",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "SPACES",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "SPACES = (\n    # https://www.htmlsymbols.xyz/punctuation-symbols/space-symbols\n    # Run in Dev Browser Console:\n    # var symbols = '';\n    # $x('//a[contains(@class, \"content-item\")]/div[@class=\"two-in-one\"][3]/span').forEach(function(el) {\n    #   symbols = symbols + \"    '\\\\u\" + el.textContent.replace('\\\\', '') + \"',\\n\";\n    # });\n    # console.log(symbols);\n    '\\u00A0',\n    '\\u0180',",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "NON_PRINTABLE",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "NON_PRINTABLE = (\n    # https://github.com/pudo/normality/blob/master/normality/cleaning.py#L10\n    # for s in range(ord('\\x00'), ord('\\x08')): print(f\"{chr(s)!r},   #  chr({s})\")\n    '\\x00',  # chr(0)\n    '\\x01',  # chr(1)\n    '\\x02',  # chr(2)\n    '\\x03',  # chr(3)\n    '\\x04',  # chr(4)\n    '\\x05',  # chr(5)\n    '\\x06',  # chr(6)",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "EXCLAMATIONS",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "EXCLAMATIONS = (\n    # HTTPS://WWW.HTMLSYMBOLS.XYZ/PUNCTUATION-SYMBOLS/EXCLAMATION-MARK\n    # '\\U0021 == '!'\n    '\\u00A1',  # \n    '\\u01C3',  # \n    '\\u203C',  # \n    '\\u2762',  # \n)\nQUESTIONS = (\n    # https://www.htmlsymbols.xyz/search?q=question",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "QUESTIONS",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "QUESTIONS = (\n    # https://www.htmlsymbols.xyz/search?q=question\n    '\\u203D',  # \n    '\\u00BF',  # \n    '\\uFF1F',  # \n)\nRE_SPACE_DOT = re.compile(r'\\s+\\.\\s*')\nRE_MANY_DASH = re.compile(r'[\\s\\-]{2,}')\nPONTUACAO = \"!\\\"#$&'*@\\\\[\\\\]^`{|}~\"\nclass Paragraphs:",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "RE_SPACE_DOT",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "RE_SPACE_DOT = re.compile(r'\\s+\\.\\s*')\nRE_MANY_DASH = re.compile(r'[\\s\\-]{2,}')\nPONTUACAO = \"!\\\"#$&'*@\\\\[\\\\]^`{|}~\"\nclass Paragraphs:\n    def __init__(self, __texto: str, separator='\\n'):\n        self.seq = __texto.splitlines()\n        self.line_num = 0    # current index into self.seq (line number)\n        self.para_num = 0    # current index into self (paragraph number)\n        # Ensure that separator string includes a line-end character at the end\n        if separator[-1:] != '\\n': separator += '\\n'",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "RE_MANY_DASH",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "RE_MANY_DASH = re.compile(r'[\\s\\-]{2,}')\nPONTUACAO = \"!\\\"#$&'*@\\\\[\\\\]^`{|}~\"\nclass Paragraphs:\n    def __init__(self, __texto: str, separator='\\n'):\n        self.seq = __texto.splitlines()\n        self.line_num = 0    # current index into self.seq (line number)\n        self.para_num = 0    # current index into self (paragraph number)\n        # Ensure that separator string includes a line-end character at the end\n        if separator[-1:] != '\\n': separator += '\\n'\n        self.separator = separator",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "PONTUACAO",
        "kind": 5,
        "importPath": "app.api.prepdoclib.clean_symbols",
        "description": "app.api.prepdoclib.clean_symbols",
        "peekOfCode": "PONTUACAO = \"!\\\"#$&'*@\\\\[\\\\]^`{|}~\"\nclass Paragraphs:\n    def __init__(self, __texto: str, separator='\\n'):\n        self.seq = __texto.splitlines()\n        self.line_num = 0    # current index into self.seq (line number)\n        self.para_num = 0    # current index into self (paragraph number)\n        # Ensure that separator string includes a line-end character at the end\n        if separator[-1:] != '\\n': separator += '\\n'\n        self.separator = separator\n    def __getitem__(self, index):",
        "detail": "app.api.prepdoclib.clean_symbols",
        "documentation": {}
    },
    {
        "label": "ComunicadoTextSplitter",
        "kind": 6,
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "peekOfCode": "class ComunicadoTextSplitter(TextSplitter):\n    \"\"\" Quebra em partes iguais textos de comunicados \"\"\"\n    def __init__(self, **kwargs: Any):\n        super().__init__(add_start_index=False, **kwargs)\n    def split_text(self, text: str) -> List[str]:\n        \"\"\" Implementao do mtodo abstrato da classe TextSplitter \"\"\"\n        __text = f\"{text}\" # tratar_linhas_texto(text)\n        chunks = []\n        while len(__text) > 0:\n            if len(__text) > self._chunk_size:",
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "tratar_linhas_texto",
        "kind": 2,
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "peekOfCode": "def tratar_linhas_texto(document: str) -> str:\n    __text = document.replace('.\\n', '.\\n\\n').replace('!\\n', '!\\n\\n').replace('?\\n', '?\\n\\n').replace(':\\n',\n                                                                                                      ':\\n\\n').replace(\n        ',\\n', ', ').replace(';\\n', ' ')\n    __text = __text.replace('\\\\s\\\\n', '')\n    __text = __text.replace('  ', ' ')\n    return __text\ndef clean_query(query: str) -> str:\n    return clean_text(query)\ndef clean_text(text: str) -> str:",
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "clean_query",
        "kind": 2,
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "peekOfCode": "def clean_query(query: str) -> str:\n    return clean_text(query)\ndef clean_text(text: str) -> str:\n    __text_parser = TextParser()\n    __text = __text_parser.clean_empty_lines_mail_sites(text)\n    __clean = re.split(r'\\s+|#\\s*', __text)\n    __text = ' '.join(__clean)\n    return __text\nclass ComunicadoTextSplitter(TextSplitter):\n    \"\"\" Quebra em partes iguais textos de comunicados \"\"\"",
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "app.api.prepdoclib.comunicado_splitter",
        "description": "app.api.prepdoclib.comunicado_splitter",
        "peekOfCode": "def clean_text(text: str) -> str:\n    __text_parser = TextParser()\n    __text = __text_parser.clean_empty_lines_mail_sites(text)\n    __clean = re.split(r'\\s+|#\\s*', __text)\n    __text = ' '.join(__clean)\n    return __text\nclass ComunicadoTextSplitter(TextSplitter):\n    \"\"\" Quebra em partes iguais textos de comunicados \"\"\"\n    def __init__(self, **kwargs: Any):\n        super().__init__(add_start_index=False, **kwargs)",
        "detail": "app.api.prepdoclib.comunicado_splitter",
        "documentation": {}
    },
    {
        "label": "ImageProcessing",
        "kind": 6,
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "peekOfCode": "class ImageProcessing:\n    def __init__(self):\n        # self.__eocrreader = easyocr.Reader(['pt','en'])\n        print(\"image-processing\")\n    def find_words_remove(self, img_data):\n        __words_remove = []\n        __whitelist = ['','STI:','STI','CCI','-','',\"N\",'IC','SICOOB']\n        __threshold2 = 10\n        for i in range(0, len(img_data['text'])):\n            __palavra = img_data['text'][i]",
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "PyTesseractParser",
        "kind": 6,
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "peekOfCode": "class PyTesseractParser(BaseBlobParser):\n    \"\"\"Carga un PDF con pdf2image y extrae texto usando pytesseract.\"\"\"\n    def __init__(self, pdf_path: str):\n        self._img_processing = ImageProcessing()\n        self._pdf_path       = pdf_path\n        self._text_parser    = TextParser()\n        self._all_letters = \" abcdefghijlmonpqrstuvxyzABCDEFGHIJLMNOPQRSTUVXYZ.,;'-0123456789\"\n    def __unicode_to_ascii(self, s: str) -> str:\n        texto = ''\n        for c in s:",
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "PyTesseractLoader",
        "kind": 6,
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "peekOfCode": "class PyTesseractLoader(BasePDFLoader):\n    def __init__(self, file_path: str) -> None:\n        self.parser = PyTesseractParser(pdf_path=file_path)\n        super().__init__(file_path)\n    def load(self) -> List[Document]:\n        return list(self.lazy_load())\n    def lazy_load(self) -> Iterator[Document]:\n        yield from self.parser.parse(None)\nif __name__ == '__main__':\n    file_path = \"./files/pdfs/0702763-79.2024.8.07.0014_0001.pdf\"",
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "pytesseract.pytesseract.tesseract_cmd",
        "kind": 5,
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "peekOfCode": "pytesseract.pytesseract.tesseract_cmd = r\"C:/Users/rogerio.rodrigues/AppData/Local/Programs/Tesseract-OCR/tesseract.exe\"\nos.environ['OMP_THREAD_LIMIT'] = '1'\nclass ImageProcessing:\n    def __init__(self):\n        # self.__eocrreader = easyocr.Reader(['pt','en'])\n        print(\"image-processing\")\n    def find_words_remove(self, img_data):\n        __words_remove = []\n        __whitelist = ['','STI:','STI','CCI','-','',\"N\",'IC','SICOOB']\n        __threshold2 = 10",
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "os.environ['OMP_THREAD_LIMIT']",
        "kind": 5,
        "importPath": "app.api.prepdoclib.image_utils",
        "description": "app.api.prepdoclib.image_utils",
        "peekOfCode": "os.environ['OMP_THREAD_LIMIT'] = '1'\nclass ImageProcessing:\n    def __init__(self):\n        # self.__eocrreader = easyocr.Reader(['pt','en'])\n        print(\"image-processing\")\n    def find_words_remove(self, img_data):\n        __words_remove = []\n        __whitelist = ['','STI:','STI','CCI','-','',\"N\",'IC','SICOOB']\n        __threshold2 = 10\n        for i in range(0, len(img_data['text'])):",
        "detail": "app.api.prepdoclib.image_utils",
        "documentation": {}
    },
    {
        "label": "TextParser",
        "kind": 6,
        "importPath": "app.api.prepdoclib.textparser",
        "description": "app.api.prepdoclib.textparser",
        "peekOfCode": "class TextParser:\n    \"\"\"Parser simples de texto.\"\"\"\n    def __clean_empty_lines(self, full_text: str) -> str:\n        \"\"\" Remove mais de 3 quebras de linhas \"\"\"\n        __full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n        __full_text = re.sub(r'(\\n\\s){3,}', '\\n\\n', __full_text)\n        __full_text = __full_text.replace('\\xa0', ' ') # Replace non-breaking spaces\n        return __full_text\n    def clean_empty_lines_mail_sites(self, texto: str) -> str:\n        # default clean",
        "detail": "app.api.prepdoclib.textparser",
        "documentation": {}
    },
    {
        "label": "check_documents",
        "kind": 2,
        "importPath": "app.api.routes.check_documents",
        "description": "app.api.routes.check_documents",
        "peekOfCode": "def check_documents(usuario_logado: Union[User | None] = InformacoesUsuario, payload: Union[CheckDocumentsPayload | None] = None) -> Any:\n    return CheckDocumentsResponse(\n        count=0,\n        documents=None\n    )",
        "detail": "app.api.routes.check_documents",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.check_documents",
        "description": "app.api.routes.check_documents",
        "peekOfCode": "router = APIRouter()\n@router.post(\"/check_documents\", dependencies=[InformacoesUsuario], response_model=CheckDocumentsResponse)\ndef check_documents(usuario_logado: Union[User | None] = InformacoesUsuario, payload: Union[CheckDocumentsPayload | None] = None) -> Any:\n    return CheckDocumentsResponse(\n        count=0,\n        documents=None\n    )",
        "detail": "app.api.routes.check_documents",
        "documentation": {}
    },
    {
        "label": "send_message",
        "kind": 2,
        "importPath": "app.api.routes.conversation",
        "description": "app.api.routes.conversation",
        "peekOfCode": "def send_message(question: str, __rag_service: ComunicadosService) -> str:\n    return __rag_service.invoke(query=question)\n# @router.post(\"/conversation\", dependencies=[InformacoesUsuario], response_model=ConversationResponse)\n@router.post(\"/conversation\", dependencies=[RagService], response_model=ConversationResponse)\ndef parent(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response = send_message(__payload.properties.question.description.strip(), __rag_service)\n    return Response(content=__response, status_code=200, media_type=\"text/plain\")\n    # return ConversationResponse(data=__response, success=True)",
        "detail": "app.api.routes.conversation",
        "documentation": {}
    },
    {
        "label": "parent",
        "kind": 2,
        "importPath": "app.api.routes.conversation",
        "description": "app.api.routes.conversation",
        "peekOfCode": "def parent(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response = send_message(__payload.properties.question.description.strip(), __rag_service)\n    return Response(content=__response, status_code=200, media_type=\"text/plain\")\n    # return ConversationResponse(data=__response, success=True)",
        "detail": "app.api.routes.conversation",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.routes.conversation",
        "description": "app.api.routes.conversation",
        "peekOfCode": "logger = logging.getLogger(__name__)\nrouter = APIRouter()\ndef send_message(question: str, __rag_service: ComunicadosService) -> str:\n    return __rag_service.invoke(query=question)\n# @router.post(\"/conversation\", dependencies=[InformacoesUsuario], response_model=ConversationResponse)\n@router.post(\"/conversation\", dependencies=[RagService], response_model=ConversationResponse)\ndef parent(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response = send_message(__payload.properties.question.description.strip(), __rag_service)\n    return Response(content=__response, status_code=200, media_type=\"text/plain\")\n    # return ConversationResponse(data=__response, success=True)",
        "detail": "app.api.routes.conversation",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.conversation",
        "description": "app.api.routes.conversation",
        "peekOfCode": "router = APIRouter()\ndef send_message(question: str, __rag_service: ComunicadosService) -> str:\n    return __rag_service.invoke(query=question)\n# @router.post(\"/conversation\", dependencies=[InformacoesUsuario], response_model=ConversationResponse)\n@router.post(\"/conversation\", dependencies=[RagService], response_model=ConversationResponse)\ndef parent(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response = send_message(__payload.properties.question.description.strip(), __rag_service)\n    return Response(content=__response, status_code=200, media_type=\"text/plain\")\n    # return ConversationResponse(data=__response, success=True)",
        "detail": "app.api.routes.conversation",
        "documentation": {}
    },
    {
        "label": "extrair_numero_cci",
        "kind": 2,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "def extrair_numero_cci(context: str) -> str:\n    _splitado = context.split('\\n')\n    _regexer  = re.compile(fr'(?<=(CCI\\s[|-]\\s)).*(?=.$)')\n    return ''.join(list(filter(_regexer.findall, _splitado)))\ndef limpar_texto(t: str) -> str:\n    t = t.replace(' \\n', '\\n').replace(' \\n', ' ').replace('\\n', ' ').replace('- \\n', '- ').replace('-\\n', '- ').replace(') \\n', ') ').replace(')\\n', ') ').replace('o \\n', 'o ').replace('o\\n', 'o ').replace('s \\n', 's ').replace('s\\n', 's ').replace('e \\n', 's ').replace('e\\n', 'e ').replace('a \\n', 'a ').replace('a\\n', 'a ').replace('r \\n', 'r ').replace('r\\n', 'r ').replace(' \\n', ' ').replace('\\n', ' ').replace('HRESTRITA', '')\n    _cleaner = CleanSymbolsProcessor()\n    return _cleaner.process_line(t)# .replace('\\n', ' '))\ndef tratar_contexto(relevant_docs: List) -> str:\n    __contexto_relevante = ''",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "limpar_texto",
        "kind": 2,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "def limpar_texto(t: str) -> str:\n    t = t.replace(' \\n', '\\n').replace(' \\n', ' ').replace('\\n', ' ').replace('- \\n', '- ').replace('-\\n', '- ').replace(') \\n', ') ').replace(')\\n', ') ').replace('o \\n', 'o ').replace('o\\n', 'o ').replace('s \\n', 's ').replace('s\\n', 's ').replace('e \\n', 's ').replace('e\\n', 'e ').replace('a \\n', 'a ').replace('a\\n', 'a ').replace('r \\n', 'r ').replace('r\\n', 'r ').replace(' \\n', ' ').replace('\\n', ' ').replace('HRESTRITA', '')\n    _cleaner = CleanSymbolsProcessor()\n    return _cleaner.process_line(t)# .replace('\\n', ' '))\ndef tratar_contexto(relevant_docs: List) -> str:\n    __contexto_relevante = ''\n    __keys = relevant_docs.keys()\n    for chave in __keys:\n        __texto               = ''.join((relevant_docs[chave])).replace('HRESTRITAH', '').replace(\"#RESTRITA#\", '').replace('\\r\\n', '\\n\\n').replace('#', '').replace('.\\n', '.\\n\\n').replace('.\\r\\n', '.\\n\\n').replace(',\\n', '.\\n\\n').replace(',\\r\\n', '.\\n\\n')\n        __contexto_relevante += f\"## Comunicado: {extrair_numero_cci(__texto)}\\n\\n\"     ",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "tratar_contexto",
        "kind": 2,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "def tratar_contexto(relevant_docs: List) -> str:\n    __contexto_relevante = ''\n    __keys = relevant_docs.keys()\n    for chave in __keys:\n        __texto               = ''.join((relevant_docs[chave])).replace('HRESTRITAH', '').replace(\"#RESTRITA#\", '').replace('\\r\\n', '\\n\\n').replace('#', '').replace('.\\n', '.\\n\\n').replace('.\\r\\n', '.\\n\\n').replace(',\\n', '.\\n\\n').replace(',\\r\\n', '.\\n\\n')\n        __contexto_relevante += f\"## Comunicado: {extrair_numero_cci(__texto)}\\n\\n\"     \n        __contexto_relevante += limpar_texto(__texto.strip())\n        __contexto_relevante += \"\\n\\n\\n\"\n    return __contexto_relevante\nasync def send_message(question: str, relevant_docs: str) -> AsyncIterable[str]:",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 2,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "def conversation(payload: Union[ConversationPayload | None] = None) -> Any:\n    __response = send_message(payload.properties.question.description.strip())\n    return StreamingResponse(__response, media_type=\"text/event-stream\")",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=3)\nembeddings    = OllamaEmbeddings(model=EMBD)\nllm           = ChatOllama(\n    model=MODEL_Q2,\n    keep_alive='1h',\n    temperature=0.4,\n    num_predict=2000,\n)\nllm_query     = ChatOllama(\n    model=MODEL_Q2,",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "logger = logging.getLogger(__name__)\nrouter = APIRouter()\nsystem_prompt = \"Voc  um assistente dedicado a responder perguntas de usurios utilizando apenas o contedo do CONTEXTO fornecido. Se voc no souber a resposta, escreva que a pergunta deve ser reformulada ou que o contexto  insuficiente. No faa comentrios. Escreva seu raciocnio passo a passo para ter certeza de que gerou a resposta correta!\"\nrag_service   = None # ComunicadosService(embeddings, text_splitter, chain, chain_q2, system_prompt, './files/pdfs/', True)\ndef extrair_numero_cci(context: str) -> str:\n    _splitado = context.split('\\n')\n    _regexer  = re.compile(fr'(?<=(CCI\\s[|-]\\s)).*(?=.$)')\n    return ''.join(list(filter(_regexer.findall, _splitado)))\ndef limpar_texto(t: str) -> str:\n    t = t.replace(' \\n', '\\n').replace(' \\n', ' ').replace('\\n', ' ').replace('- \\n', '- ').replace('-\\n', '- ').replace(') \\n', ') ').replace(')\\n', ') ').replace('o \\n', 'o ').replace('o\\n', 'o ').replace('s \\n', 's ').replace('s\\n', 's ').replace('e \\n', 's ').replace('e\\n', 'e ').replace('a \\n', 'a ').replace('a\\n', 'a ').replace('r \\n', 'r ').replace('r\\n', 'r ').replace(' \\n', ' ').replace('\\n', ' ').replace('HRESTRITA', '')",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "router = APIRouter()\nsystem_prompt = \"Voc  um assistente dedicado a responder perguntas de usurios utilizando apenas o contedo do CONTEXTO fornecido. Se voc no souber a resposta, escreva que a pergunta deve ser reformulada ou que o contexto  insuficiente. No faa comentrios. Escreva seu raciocnio passo a passo para ter certeza de que gerou a resposta correta!\"\nrag_service   = None # ComunicadosService(embeddings, text_splitter, chain, chain_q2, system_prompt, './files/pdfs/', True)\ndef extrair_numero_cci(context: str) -> str:\n    _splitado = context.split('\\n')\n    _regexer  = re.compile(fr'(?<=(CCI\\s[|-]\\s)).*(?=.$)')\n    return ''.join(list(filter(_regexer.findall, _splitado)))\ndef limpar_texto(t: str) -> str:\n    t = t.replace(' \\n', '\\n').replace(' \\n', ' ').replace('\\n', ' ').replace('- \\n', '- ').replace('-\\n', '- ').replace(') \\n', ') ').replace(')\\n', ') ').replace('o \\n', 'o ').replace('o\\n', 'o ').replace('s \\n', 's ').replace('s\\n', 's ').replace('e \\n', 's ').replace('e\\n', 'e ').replace('a \\n', 'a ').replace('a\\n', 'a ').replace('r \\n', 'r ').replace('r\\n', 'r ').replace(' \\n', ' ').replace('\\n', ' ').replace('HRESTRITA', '')\n    _cleaner = CleanSymbolsProcessor()",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "system_prompt",
        "kind": 5,
        "importPath": "app.api.routes.conversation2",
        "description": "app.api.routes.conversation2",
        "peekOfCode": "system_prompt = \"Voc  um assistente dedicado a responder perguntas de usurios utilizando apenas o contedo do CONTEXTO fornecido. Se voc no souber a resposta, escreva que a pergunta deve ser reformulada ou que o contexto  insuficiente. No faa comentrios. Escreva seu raciocnio passo a passo para ter certeza de que gerou a resposta correta!\"\nrag_service   = None # ComunicadosService(embeddings, text_splitter, chain, chain_q2, system_prompt, './files/pdfs/', True)\ndef extrair_numero_cci(context: str) -> str:\n    _splitado = context.split('\\n')\n    _regexer  = re.compile(fr'(?<=(CCI\\s[|-]\\s)).*(?=.$)')\n    return ''.join(list(filter(_regexer.findall, _splitado)))\ndef limpar_texto(t: str) -> str:\n    t = t.replace(' \\n', '\\n').replace(' \\n', ' ').replace('\\n', ' ').replace('- \\n', '- ').replace('-\\n', '- ').replace(') \\n', ') ').replace(')\\n', ') ').replace('o \\n', 'o ').replace('o\\n', 'o ').replace('s \\n', 's ').replace('s\\n', 's ').replace('e \\n', 's ').replace('e\\n', 'e ').replace('a \\n', 'a ').replace('a\\n', 'a ').replace('r \\n', 'r ').replace('r\\n', 'r ').replace(' \\n', ' ').replace('\\n', ' ').replace('HRESTRITA', '')\n    _cleaner = CleanSymbolsProcessor()\n    return _cleaner.process_line(t)# .replace('\\n', ' '))",
        "detail": "app.api.routes.conversation2",
        "documentation": {}
    },
    {
        "label": "conversation_with_sources",
        "kind": 2,
        "importPath": "app.api.routes.conversation_normativos",
        "description": "app.api.routes.conversation_normativos",
        "peekOfCode": "def conversation_with_sources(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response, __documents = __rag_service.invoke_with_sources(__payload.properties.question.description.strip())\n    sources = list()\n    for __doc in __documents:\n        __name         = str(__doc.metadata['source'])\n        __name_splited = __name.split('/')\n        __name         = __name_splited[len(__name_splited) - 1]\n        # verifica se j existe na lista a fonte\n        exists = False\n        #for __i in sources:",
        "detail": "app.api.routes.conversation_normativos",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.routes.conversation_normativos",
        "description": "app.api.routes.conversation_normativos",
        "peekOfCode": "logger = logging.getLogger(__name__)\nrouter = APIRouter()\n@router.post(\"/conversation-with-sources\", dependencies=[RagService], response_model=NormativosResponse)\ndef conversation_with_sources(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response, __documents = __rag_service.invoke_with_sources(__payload.properties.question.description.strip())\n    sources = list()\n    for __doc in __documents:\n        __name         = str(__doc.metadata['source'])\n        __name_splited = __name.split('/')\n        __name         = __name_splited[len(__name_splited) - 1]",
        "detail": "app.api.routes.conversation_normativos",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.conversation_normativos",
        "description": "app.api.routes.conversation_normativos",
        "peekOfCode": "router = APIRouter()\n@router.post(\"/conversation-with-sources\", dependencies=[RagService], response_model=NormativosResponse)\ndef conversation_with_sources(__payload: Union[ConversationPayload | None] = None, __rag_service: ComunicadosService | None = RagService) -> Any:\n    __response, __documents = __rag_service.invoke_with_sources(__payload.properties.question.description.strip())\n    sources = list()\n    for __doc in __documents:\n        __name         = str(__doc.metadata['source'])\n        __name_splited = __name.split('/')\n        __name         = __name_splited[len(__name_splited) - 1]\n        # verifica se j existe na lista a fonte",
        "detail": "app.api.routes.conversation_normativos",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.routes.data_analysis",
        "description": "app.api.routes.data_analysis",
        "peekOfCode": "logger = logging.getLogger(__name__)\nrouter = APIRouter()\ndef __fake_data_streamer(__mensagem: str):\n    __data_service = DataAnalysisService()\n    __query, __propt = __data_service.chat(__mensagem)\n    for part in ollama.chat(\n        model='qwen2',\n        messages=[\n            {'role': 'system', 'content': __propt},\n            {'role': 'user', 'content': __query}",
        "detail": "app.api.routes.data_analysis",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.data_analysis",
        "description": "app.api.routes.data_analysis",
        "peekOfCode": "router = APIRouter()\ndef __fake_data_streamer(__mensagem: str):\n    __data_service = DataAnalysisService()\n    __query, __propt = __data_service.chat(__mensagem)\n    for part in ollama.chat(\n        model='qwen2',\n        messages=[\n            {'role': 'system', 'content': __propt},\n            {'role': 'user', 'content': __query}\n        ],",
        "detail": "app.api.routes.data_analysis",
        "documentation": {}
    },
    {
        "label": "get_signed_url",
        "kind": 2,
        "importPath": "app.api.routes.get_signed_url",
        "description": "app.api.routes.get_signed_url",
        "peekOfCode": "def get_signed_url(usuario_logado: User | None = InformacoesUsuario, pdf: str | None = None) -> Any:\n    print(usuario_logado)\n    print(pdf)\n    \"\"\"\n    Retorna as urls pr-assinadas\n    \"\"\"\n    return SignedUrls(presigned_urls=[f'http://localhost:8000/upload/{pdf}'])",
        "detail": "app.api.routes.get_signed_url",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.get_signed_url",
        "description": "app.api.routes.get_signed_url",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/get-signed-url\", dependencies=[InformacoesUsuario], response_model=SignedUrls)\ndef get_signed_url(usuario_logado: User | None = InformacoesUsuario, pdf: str | None = None) -> Any:\n    print(usuario_logado)\n    print(pdf)\n    \"\"\"\n    Retorna as urls pr-assinadas\n    \"\"\"\n    return SignedUrls(presigned_urls=[f'http://localhost:8000/upload/{pdf}'])",
        "detail": "app.api.routes.get_signed_url",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.load_data",
        "description": "app.api.routes.load_data",
        "peekOfCode": "router = APIRouter()\nasync def proccess_task(__rag_service: ComunicadosService | None) -> None:\n    try:\n        await __rag_service.load_data()\n    except Exception as e:\n        print(\"ERRO\")\n        print(e)\n@router.get(\"/load-data\", dependencies=[RagService])\nasync def load_data(background_tasks: BackgroundTasks, __rag_service: ComunicadosService | None = RagService) -> Any:\n    background_tasks.add_task(proccess_task, __rag_service)",
        "detail": "app.api.routes.load_data",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.routes.parent",
        "description": "app.api.routes.parent",
        "peekOfCode": "logger = logging.getLogger(__name__)\nrouter = APIRouter()\nasync def send_message(__question: str, __rag_service: ComunicadosService) -> AsyncIterable[str]:\n    __callback = AsyncIteratorCallbackHandler()\n    __rag_service.set_callbacks([__callback])\n    async def wrap_done(fn: Awaitable, event: asyncio.Event):\n        \"\"\"Wrap an awaitable with a event to signal when it's done or an exception is raised.\"\"\"\n        try:\n            await fn\n        except Exception as e:",
        "detail": "app.api.routes.parent",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.parent",
        "description": "app.api.routes.parent",
        "peekOfCode": "router = APIRouter()\nasync def send_message(__question: str, __rag_service: ComunicadosService) -> AsyncIterable[str]:\n    __callback = AsyncIteratorCallbackHandler()\n    __rag_service.set_callbacks([__callback])\n    async def wrap_done(fn: Awaitable, event: asyncio.Event):\n        \"\"\"Wrap an awaitable with a event to signal when it's done or an exception is raised.\"\"\"\n        try:\n            await fn\n        except Exception as e:\n            print(f\"Caught exception: {e}\")",
        "detail": "app.api.routes.parent",
        "documentation": {}
    },
    {
        "label": "user_control_service",
        "kind": 2,
        "importPath": "app.api.routes.user_control_service",
        "description": "app.api.routes.user_control_service",
        "peekOfCode": "def user_control_service(payload: Union[UserControlServicePayload | None] = None) -> Any:\n    print(payload)\n    __storage = LocalStorage()\n    __storage.save(filename=payload.file_key.replace('.pdf', ''), data=payload.service.encode('utf-8'))\n    return UserControlServiceResponse(\n        message=f\"O documento {payload.file_key} foi salvo com sucesso na feature {payload.service}\"\n    )",
        "detail": "app.api.routes.user_control_service",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.routes.user_control_service",
        "description": "app.api.routes.user_control_service",
        "peekOfCode": "router = APIRouter()\n# @router.post(\"/user_control_service\", dependencies=[InformacoesUsuario], response_model=UserControlServiceResponse)\n# def user_control_service(usuario_logado: Union[User | None] = InformacoesUsuario, payload: Union[UserControlServicePayload | None] = None) -> Any:\n@router.post(\"/user_control_service\", response_model=UserControlServiceResponse)\ndef user_control_service(payload: Union[UserControlServicePayload | None] = None) -> Any:\n    print(payload)\n    __storage = LocalStorage()\n    __storage.save(filename=payload.file_key.replace('.pdf', ''), data=payload.service.encode('utf-8'))\n    return UserControlServiceResponse(\n        message=f\"O documento {payload.file_key} foi salvo com sucesso na feature {payload.service}\"",
        "detail": "app.api.routes.user_control_service",
        "documentation": {}
    },
    {
        "label": "ComunicadosService",
        "kind": 6,
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "peekOfCode": "class ComunicadosService():\n    \"\"\" Classe responsvel por converter arquivos PDF em Imagens.\n        Transforma as mesmas em textos, e inclui em base de dados (memria).\n    \"\"\"\n    def __init__(\n        self,\n        text_splitter: CharacterTextSplitter,\n        llm: Union[ChatOllama | None],\n        system_prompt: Union[str, None] = None,\n        folder: Union[str, None] = None,",
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "unicode_to_ascii",
        "kind": 2,
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "peekOfCode": "def unicode_to_ascii(s: str) -> str:\n    texto = ''\n    for c in s:\n        if c in all_letters:\n            texto += c\n    return ''.join(texto)\ndef normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())\n    return s.strip()\nclass ComunicadosService():",
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "normalize_string",
        "kind": 2,
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "peekOfCode": "def normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())\n    return s.strip()\nclass ComunicadosService():\n    \"\"\" Classe responsvel por converter arquivos PDF em Imagens.\n        Transforma as mesmas em textos, e inclui em base de dados (memria).\n    \"\"\"\n    def __init__(\n        self,\n        text_splitter: CharacterTextSplitter,",
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "langchain.debug",
        "kind": 5,
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "peekOfCode": "langchain.debug = False\nall_letters = \" abcdefghijlmonpqrstuvxyzABCDEFGHIJLMNOPQRSTUVXYZ.,;'-0123456789\"\ndef unicode_to_ascii(s: str) -> str:\n    texto = ''\n    for c in s:\n        if c in all_letters:\n            texto += c\n    return ''.join(texto)\ndef normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())",
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "all_letters",
        "kind": 5,
        "importPath": "app.api.services.comunicados_service",
        "description": "app.api.services.comunicados_service",
        "peekOfCode": "all_letters = \" abcdefghijlmonpqrstuvxyzABCDEFGHIJLMNOPQRSTUVXYZ.,;'-0123456789\"\ndef unicode_to_ascii(s: str) -> str:\n    texto = ''\n    for c in s:\n        if c in all_letters:\n            texto += c\n    return ''.join(texto)\ndef normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())\n    return s.strip()",
        "detail": "app.api.services.comunicados_service",
        "documentation": {}
    },
    {
        "label": "DataAnalysisService",
        "kind": 6,
        "importPath": "app.api.services.data_analysis_service",
        "description": "app.api.services.data_analysis_service",
        "peekOfCode": "class DataAnalysisService():\n    \"\"\" Classe responsvel por converter arquivos PDF em Imagens.\n        Transforma as mesmas em textos, e inclui em base de dados (memria).\n    \"\"\"\n    def __init__(self):\n        print('Data Analysis Service')\n    def __build_llama_prompt(self, data_frame: str) -> str:\n        return F\"\"\"\n            Conjunto de dados:\n            <dataframe>",
        "detail": "app.api.services.data_analysis_service",
        "documentation": {}
    },
    {
        "label": "MetadataService",
        "kind": 6,
        "importPath": "app.api.services.metadata_service",
        "description": "app.api.services.metadata_service",
        "peekOfCode": "class MetadataService:\n    def __init__(self):\n        self.__model          = 'qwen2'\n        self.__summary_prompt = \"Voc  um assistente especialista em resumo de documentos. Sua tarefa  fazer um resumo claro e conciso de documentos, foque em aspectos como assunto, nomes de pessoas, empresas ou instituies financeiras, CPF e CNPJ, identificador do comunicado (CCI) caso exista, objetivo do documento, funcionalidades descritas no documento, valores, condies, tipos de contas, quem assina o documento, identifique assinaturas digitais caso existam. No acrescente nenhum conhecimento prvio, nota ou sugesto.\"\n        self.__summary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valor da dvida, requerentes, requeridos, as partes e objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto.\"\n        self.__keys_prompt    = \"Voc  um assistente dedicado a identificar e extrair palavras-chave de um documento. Extraia entre 3 e 5 palavras-chave, pois elas sero utilizadas pela rea administrativa para buscar esse mesmo documento futuramente.\"\n        self.__multi_query    = \"Voc  um assistente dedicado a identificar e criar perguntas com base no texto de um documento. Crie apenas 2 perguntas sobre o contedo do documento fornecido.\"\n        self.__text_parser = TextParser()\n        self.__output      = StrOutputParser()\n        self.__llm         = ChatOllama(",
        "detail": "app.api.services.metadata_service",
        "documentation": {}
    },
    {
        "label": "api_router",
        "kind": 5,
        "importPath": "app.api.main",
        "description": "app.api.main",
        "peekOfCode": "api_router = APIRouter()\n# endpoints\napi_router.include_router(get_signed_url.router, tags=[\"get-signed-url\"])\napi_router.include_router(user_control_service.router, tags=[\"user-control-service\"])\napi_router.include_router(check_documents.router, tags=[\"check-documents\"])\napi_router.include_router(conversation.router, tags=[\"conversation\"])\napi_router.include_router(conversation2.router, tags=[\"conversation-tiny\"])\napi_router.include_router(parent.router, tags=[\"conversation-parent\"])\napi_router.include_router(data_analysis.router, tags=[\"data-analysis\"])\napi_router.include_router(conversation_normativos.router, tags=[\"conversation-with-sources\"])",
        "detail": "app.api.main",
        "documentation": {}
    },
    {
        "label": "BaseStorage",
        "kind": 6,
        "importPath": "app.storage.base_storage",
        "description": "app.storage.base_storage",
        "peekOfCode": "class BaseStorage(ABC):\n    \"\"\"File storage.\"\"\"\n    def __init__(self):\n        self.app = None\n    @abstractmethod\n    def save(self, filename: str, data):\n        raise NotImplementedError\n    @abstractmethod\n    def load_once(self, filename: str) -> bytes:\n        raise NotImplementedError",
        "detail": "app.storage.base_storage",
        "documentation": {}
    },
    {
        "label": "LocalStorage",
        "kind": 6,
        "importPath": "app.storage.local_storage",
        "description": "app.storage.local_storage",
        "peekOfCode": "class LocalStorage(BaseStorage):\n    \"\"\"Implementao para o local storage.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.folder = '/home/rogerio_rodrigues/python-workspace/rag_python/local_storage/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/local_storage/'\n        # self.folder = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/local_storage/'\n        print(f\"diretrio storage: {self.folder}\")\n    def save(self, filename: str, data):\n        if not self.folder or self.folder.endswith(\"/\"):\n            filename = self.folder + filename",
        "detail": "app.storage.local_storage",
        "documentation": {}
    },
    {
        "label": "storage",
        "kind": 5,
        "importPath": "app.storage.local_storage",
        "description": "app.storage.local_storage",
        "peekOfCode": "storage = LocalStorage()",
        "detail": "app.storage.local_storage",
        "documentation": {}
    },
    {
        "label": "get_memory_history",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_memory_history() -> ConversationBufferMemory:\n    \"\"\" \\nCarrega a memria de conversao\\n \"\"\"\n    __memory = ConversationBufferMemory(\n        chat_memory=ChatMessageHistory(),\n        memory_key='chat_history',\n        output_key='answer',\n        return_messages=True\n    )\n    return __memory\ndef get_text_splitter() -> Union[ComunicadoTextSplitter, None]:",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_text_splitter",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_text_splitter() -> Union[ComunicadoTextSplitter, None]:\n    __splitter = ComunicadoTextSplitter(chunk_size=4096, chunk_overlap=100)\n    return __splitter\ndef get_chat_prompt() -> Union[ChatPromptTemplate, None]:\n    __chat_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", config_system_prompt),\n            (\"user\", \"{question}\"),\n            (\"user\", \"**summary**\\n\\n{summaries}\\n\\n\\n#### Contexto ####\\n\\n{context}\\n\\n\"),\n        ]",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_chat_prompt",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_chat_prompt() -> Union[ChatPromptTemplate, None]:\n    __chat_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", config_system_prompt),\n            (\"user\", \"{question}\"),\n            (\"user\", \"**summary**\\n\\n{summaries}\\n\\n\\n#### Contexto ####\\n\\n{context}\\n\\n\"),\n        ]\n    )\n    return __chat_prompt\n@functools.cache",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_ollama_embeddings_basic",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" \\nLLM para embeddings\\n \"\"\"\n    __embed = OllamaEmbeddings(model=CONFIG_EMBD)\n    return __embed\n@functools.cache\ndef get_ollama_embeddings() -> Union[OllamaEmbeddings, None]:\n    \"\"\" \\nLLM para embeddings\\n \"\"\"\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    return __embed\n@functools.cache",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_ollama_embeddings",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_ollama_embeddings() -> Union[OllamaEmbeddings, None]:\n    \"\"\" \\nLLM para embeddings\\n \"\"\"\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    return __embed\n@functools.cache\ndef get_duckdb_vector_store_basic() -> Union[DuckDB, None]:\n    \"\"\" \\nCria o vectorstore com DUCKDB\\n \"\"\"\n    __vector_store = DuckDB(embedding=get_ollama_embeddings_basic())\n    return __vector_store\ndef get_weaviate_vector_store() -> Union[Tuple[WeaviateVectorStore, WeaviateClient], None]:",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_duckdb_vector_store_basic",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_duckdb_vector_store_basic() -> Union[DuckDB, None]:\n    \"\"\" \\nCria o vectorstore com DUCKDB\\n \"\"\"\n    __vector_store = DuckDB(embedding=get_ollama_embeddings_basic())\n    return __vector_store\ndef get_weaviate_vector_store() -> Union[Tuple[WeaviateVectorStore, WeaviateClient], None]:\n    \"\"\" \\nCria o vectorstore com WEAVIATE\\n \"\"\"\n    \"\"\"__weaviate_client = weaviate.connect_to_local(host='127.0.0.1', port=8079, grpc_port=50060)\n    __vector_store = WeaviateVectorStore(client=__weaviate_client, index_name=\"Doc_Jur\", text_key=\"page_content\", embedding=get_ollama_embeddings())\n    print(__vector_store)\n    return __vector_store, __weaviate_client\"\"\"",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_weaviate_vector_store",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_weaviate_vector_store() -> Union[Tuple[WeaviateVectorStore, WeaviateClient], None]:\n    \"\"\" \\nCria o vectorstore com WEAVIATE\\n \"\"\"\n    \"\"\"__weaviate_client = weaviate.connect_to_local(host='127.0.0.1', port=8079, grpc_port=50060)\n    __vector_store = WeaviateVectorStore(client=__weaviate_client, index_name=\"Doc_Jur\", text_key=\"page_content\", embedding=get_ollama_embeddings())\n    print(__vector_store)\n    return __vector_store, __weaviate_client\"\"\"\n    return None\n# @functools.cache\ndef get_mongodb_vector_store() -> Union[MongoDBAtlasVectorSearch, None]:\n    \"\"\" \\nCria o vectorstore com duckdb\\n \"\"\"",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_mongodb_vector_store",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_mongodb_vector_store() -> Union[MongoDBAtlasVectorSearch, None]:\n    \"\"\" \\nCria o vectorstore com duckdb\\n \"\"\"\n    try:\n        __mongo_client = MongoClient(\"mongodb+srv://rogerioalves21:cCtExPYYxjDONME9@lidcluster.h0lg3.mongodb.net/?retryWrites=true&w=majority&appName=LidCluster\")# \"mongodb://localhost:27017/?appname=SicoobLid&directConnection=true&ssl=false\")\n        __collection = __mongo_client[\"lid\"][\"sicoob-collection\"]\n        __vector_store = MongoDBAtlasVectorSearch(collection=__collection, embedding=get_ollama_embeddings(), index_name=\"vector_index\", relevance_score_fn=\"cosine\")\n        return __vector_store\n    except:\n        print(\"Sem mongo DB\")\n    return None",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_memory_store",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_memory_store() -> Union[InMemoryStore, None]:\n    \"\"\" Cria a store em memria \"\"\"\n    __store = InMemoryStore()\n    return __store\n@functools.cache\ndef get_memory_db() -> Union[DocArrayInMemorySearch, None]:\n    \"\"\" Cria o banco de dados em memria \"\"\"\n    __data_base = DocArrayInMemorySearch.from_params(\n        embedding=get_ollama_embeddings(),\n        metric=\"euclidian_dist\",",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_memory_db",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_memory_db() -> Union[DocArrayInMemorySearch, None]:\n    \"\"\" Cria o banco de dados em memria \"\"\"\n    __data_base = DocArrayInMemorySearch.from_params(\n        embedding=get_ollama_embeddings(),\n        metric=\"euclidian_dist\",\n    )\n    return __data_base\ndef get_chat_ollama_client() -> Union[ChatOllama, None]:\n    \"\"\" Instncia do cliente para os LLMs do ollama \"\"\"\n    __llm = ChatOllama(",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_chat_ollama_client",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_chat_ollama_client() -> Union[ChatOllama, None]:\n    \"\"\" Instncia do cliente para os LLMs do ollama \"\"\"\n    __llm = ChatOllama(\n        model=MODEL_GEMMA,\n        keep_alive=0,\n        temperature=0.3,\n        num_ctx=4096,\n        num_predict=8192,\n        repeat_penalty=1.18,\n        num_gpu= 0,",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_rag_service",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_rag_service(\n        text_splitter: Annotated[ComunicadoTextSplitter, Depends(get_text_splitter)],\n        llm_streaming: Annotated[ChatOllama, Depends(get_chat_ollama_client)],\n        chat_prompt: Annotated[ChatPromptTemplate, Depends(get_chat_prompt)],\n        memory_history: Annotated[ChatPromptTemplate, Depends(get_memory_history)],\n        memory_data_base: Annotated[DocArrayInMemorySearch, Depends(get_memory_db)],\n        memory_store: Annotated[InMemoryStore, Depends(get_memory_store)],\n        mongodb_vector_storage: Annotated[MongoDBAtlasVectorSearch, Depends(get_mongodb_vector_store)],\n        duckdb_vector_storage_basic: Annotated[DuckDB, Depends(get_duckdb_vector_store_basic)],\n        embeddings: Annotated[OllamaEmbeddings, Depends(get_ollama_embeddings)],",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "create_user",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def create_user(token: Union[str, None]) -> User:\n    return User(\n        cpfCnpj=\"00875981160\",\n        cooperativa=\"0300\",\n        descricao=\"Rogrio Alves Rodrigues\",\n        email=\"rogerioalves21@gmail.com\",\n        login=\"rogerio.rodrigues\",\n        instituicaoOrigem=\"2\")\n    \"\"\"return User(\n        cpfCnpj=payload['cpfCnpj'],",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "get_dados_usuario",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def get_dados_usuario(token: Union[str, None]) -> User:\n    return create_user(token)\n    \"\"\"try:\n        logger.info(f'JWT\\n{token}')\n        headers = {\n            'Authorization': f'Bearer {token}',\n            'accept': '*/*',\n            'client_id': client_id,\n            'Content-Type': 'application/json'\n        }",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "do_login",
        "kind": 2,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "def do_login(X_JWT_Assertion: Annotated[Union[str, None], Header()] = None) -> User:\n    logger.info('do_login')\n    # if X_JWT_Assertion is None:\n    #    raise HTTPException(\n    #        status_code=status.HTTP_401_UNAUTHORIZED,\n    #        detail=\"Usurio no autenticado\",\n    #    )\n    return get_dados_usuario(None) # X_JWT_Assertion)\nInformacoesUsuario: User | None = Depends(do_login)\nRagService: ComunicadosService | None = Depends(get_rag_service)",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# VARIVEIS PARA VALIDAO DO TOKEN RHSSO\napi_userinfo = 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo'\nclient_id    = 'lid'\nCONFIG_EMBDBERT = 'qwen2' # 'paraphrase-multilingual'\nCONFIG_EMBD  = 'qwen2' # 'mxbai-embed-large'\nMODEL_MISTRAL = 'qwen2'\nMODEL_LLAMA  = 'qwen2'\nMODEL_GEMMA  = 'qwen2'\nconfig_system_prompt = \"Voc  um assistente prestativo do Banco Sicoob, dedicado a responder perguntas utilizando somente o CONTEXTO fornecido. Se no for possvel encontrar a resposta no contexto, responda \\\"O contexto fornecido  insuficiente!\\\". No utilize conhecimento prvio. Antes de escrever sua resposta final, lembre que a resposta deve ser no idioma portugus.\"",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "api_userinfo",
        "kind": 5,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "api_userinfo = 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo'\nclient_id    = 'lid'\nCONFIG_EMBDBERT = 'qwen2' # 'paraphrase-multilingual'\nCONFIG_EMBD  = 'qwen2' # 'mxbai-embed-large'\nMODEL_MISTRAL = 'qwen2'\nMODEL_LLAMA  = 'qwen2'\nMODEL_GEMMA  = 'qwen2'\nconfig_system_prompt = \"Voc  um assistente prestativo do Banco Sicoob, dedicado a responder perguntas utilizando somente o CONTEXTO fornecido. Se no for possvel encontrar a resposta no contexto, responda \\\"O contexto fornecido  insuficiente!\\\". No utilize conhecimento prvio. Antes de escrever sua resposta final, lembre que a resposta deve ser no idioma portugus.\"\n@functools.cache\ndef get_memory_history() -> ConversationBufferMemory:",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "CONFIG_EMBDBERT",
        "kind": 5,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "CONFIG_EMBDBERT = 'qwen2' # 'paraphrase-multilingual'\nCONFIG_EMBD  = 'qwen2' # 'mxbai-embed-large'\nMODEL_MISTRAL = 'qwen2'\nMODEL_LLAMA  = 'qwen2'\nMODEL_GEMMA  = 'qwen2'\nconfig_system_prompt = \"Voc  um assistente prestativo do Banco Sicoob, dedicado a responder perguntas utilizando somente o CONTEXTO fornecido. Se no for possvel encontrar a resposta no contexto, responda \\\"O contexto fornecido  insuficiente!\\\". No utilize conhecimento prvio. Antes de escrever sua resposta final, lembre que a resposta deve ser no idioma portugus.\"\n@functools.cache\ndef get_memory_history() -> ConversationBufferMemory:\n    \"\"\" \\nCarrega a memria de conversao\\n \"\"\"\n    __memory = ConversationBufferMemory(",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "MODEL_MISTRAL",
        "kind": 5,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "MODEL_MISTRAL = 'qwen2'\nMODEL_LLAMA  = 'qwen2'\nMODEL_GEMMA  = 'qwen2'\nconfig_system_prompt = \"Voc  um assistente prestativo do Banco Sicoob, dedicado a responder perguntas utilizando somente o CONTEXTO fornecido. Se no for possvel encontrar a resposta no contexto, responda \\\"O contexto fornecido  insuficiente!\\\". No utilize conhecimento prvio. Antes de escrever sua resposta final, lembre que a resposta deve ser no idioma portugus.\"\n@functools.cache\ndef get_memory_history() -> ConversationBufferMemory:\n    \"\"\" \\nCarrega a memria de conversao\\n \"\"\"\n    __memory = ConversationBufferMemory(\n        chat_memory=ChatMessageHistory(),\n        memory_key='chat_history',",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "config_system_prompt",
        "kind": 5,
        "importPath": "app.config",
        "description": "app.config",
        "peekOfCode": "config_system_prompt = \"Voc  um assistente prestativo do Banco Sicoob, dedicado a responder perguntas utilizando somente o CONTEXTO fornecido. Se no for possvel encontrar a resposta no contexto, responda \\\"O contexto fornecido  insuficiente!\\\". No utilize conhecimento prvio. Antes de escrever sua resposta final, lembre que a resposta deve ser no idioma portugus.\"\n@functools.cache\ndef get_memory_history() -> ConversationBufferMemory:\n    \"\"\" \\nCarrega a memria de conversao\\n \"\"\"\n    __memory = ConversationBufferMemory(\n        chat_memory=ChatMessageHistory(),\n        memory_key='chat_history',\n        output_key='answer',\n        return_messages=True\n    )",
        "detail": "app.config",
        "documentation": {}
    },
    {
        "label": "langchain.verbose",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "langchain.verbose = True\nsess_options = rt.SessionOptions()\nsess_options.enable_profiling = True\nsess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options = rt.SessionOptions()\nsess_options.enable_profiling = True\nsess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.enable_profiling",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.enable_profiling = True\nsess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.log_severity_level",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local\nos.environ.setdefault('API_USERINFO', 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo')",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.execution_mode",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local\nos.environ.setdefault('API_USERINFO', 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo')\nos.environ.setdefault('CLIENT_ID', 'lid')",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.graph_optimization_level",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local\nos.environ.setdefault('API_USERINFO', 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo')\nos.environ.setdefault('CLIENT_ID', 'lid')\n# instancia a aplicao",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.inter_op_num_threads",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local\nos.environ.setdefault('API_USERINFO', 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo')\nos.environ.setdefault('CLIENT_ID', 'lid')\n# instancia a aplicao\napp = FastAPI(",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "sess_options.intra_op_num_threads",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "sess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\n# set_llm_cache(SQLiteCache())\nset_debug(True)\n# seta as variveis local\nos.environ.setdefault('API_USERINFO', 'https://api-sisbr-ti.homologacao.com.br/user-info/v2/userinfo')\nos.environ.setdefault('CLIENT_ID', 'lid')\n# instancia a aplicao\napp = FastAPI(\n    debug=True,",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app = FastAPI(\n    debug=True,\n    title='lid-sisbr-backoffice',\n)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n# habilita o CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=cors_origins,\n    allow_credentials=True,",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "SignedUrls",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class SignedUrls(BaseModel):\n    presigned_urls: Union[list[str] | None] = None\nclass User(BaseModel):\n    cooperativa: Union[str | None] = None\n    email: Union[str | None] = None\n    login: Union[str | None] = None\n    descricao: Union[str | None] = None\n    cpfCnpj: Union[str | None] = None\n    instituicaoOrigem: Union[str | None] = None\nclass UserControlServicePayload(BaseModel):",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class User(BaseModel):\n    cooperativa: Union[str | None] = None\n    email: Union[str | None] = None\n    login: Union[str | None] = None\n    descricao: Union[str | None] = None\n    cpfCnpj: Union[str | None] = None\n    instituicaoOrigem: Union[str | None] = None\nclass UserControlServicePayload(BaseModel):\n    file_key: Union[str | None] = None\n    service: Union[str | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "UserControlServicePayload",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class UserControlServicePayload(BaseModel):\n    file_key: Union[str | None] = None\n    service: Union[str | None] = None\nclass UserControlServiceResponse(BaseModel):\n    message: Union[str | None] = None\nclass DocumentContent(BaseModel):\n    _id: Union[str | None] = None\n    date_time: Union[str | None] = None\n    expireAt: Union[str | None] = None\n    embeddings: Union[list[list[float]] | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "UserControlServiceResponse",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class UserControlServiceResponse(BaseModel):\n    message: Union[str | None] = None\nclass DocumentContent(BaseModel):\n    _id: Union[str | None] = None\n    date_time: Union[str | None] = None\n    expireAt: Union[str | None] = None\n    embeddings: Union[list[list[float]] | None] = None\n    file_content: Union[str | None] = None\n    file_key: Union[str | None] = None\n    service: Union[str | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "DocumentContent",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class DocumentContent(BaseModel):\n    _id: Union[str | None] = None\n    date_time: Union[str | None] = None\n    expireAt: Union[str | None] = None\n    embeddings: Union[list[list[float]] | None] = None\n    file_content: Union[str | None] = None\n    file_key: Union[str | None] = None\n    service: Union[str | None] = None\n    user_id: Union[str | None] = None\nclass CheckDocumentsPayload(BaseModel):",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "CheckDocumentsPayload",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class CheckDocumentsPayload(BaseModel):\n    file_key: Union[str | None] = None\n    service: Union[str | None] = None\nclass CheckDocumentsResponse(BaseModel):\n    count: Union[int | None] = None\n    documents: Union[list[DocumentContent] | None] = None\nclass DescriptionModel(BaseModel):\n    type: Union[str | None] = None\n    description: Union[str | None] = None\nclass PropertiesModel(BaseModel):",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "CheckDocumentsResponse",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class CheckDocumentsResponse(BaseModel):\n    count: Union[int | None] = None\n    documents: Union[list[DocumentContent] | None] = None\nclass DescriptionModel(BaseModel):\n    type: Union[str | None] = None\n    description: Union[str | None] = None\nclass PropertiesModel(BaseModel):\n    question: Union[DescriptionModel | None] = None\n    service: Union[DescriptionModel | None] = None\n    model: Union[DescriptionModel | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "DescriptionModel",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class DescriptionModel(BaseModel):\n    type: Union[str | None] = None\n    description: Union[str | None] = None\nclass PropertiesModel(BaseModel):\n    question: Union[DescriptionModel | None] = None\n    service: Union[DescriptionModel | None] = None\n    model: Union[DescriptionModel | None] = None\nclass SourceModel(BaseModel):\n    name: Union[str | None] = None\n    link: Union[str | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "PropertiesModel",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class PropertiesModel(BaseModel):\n    question: Union[DescriptionModel | None] = None\n    service: Union[DescriptionModel | None] = None\n    model: Union[DescriptionModel | None] = None\nclass SourceModel(BaseModel):\n    name: Union[str | None] = None\n    link: Union[str | None] = None\n    page: Union[int | None] = 0\n    summary: Union[str | None] = 0\n    topic_suggestions: Union[str, None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "SourceModel",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class SourceModel(BaseModel):\n    name: Union[str | None] = None\n    link: Union[str | None] = None\n    page: Union[int | None] = 0\n    summary: Union[str | None] = 0\n    topic_suggestions: Union[str, None] = None\nclass ConversationPayload(BaseModel):\n    type: Union[str | None] = None\n    start_date: Union[str | None] = None\n    end_date: Union[str | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationPayload",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class ConversationPayload(BaseModel):\n    type: Union[str | None] = None\n    start_date: Union[str | None] = None\n    end_date: Union[str | None] = None\n    properties: Union[PropertiesModel | None] = None\nclass ConversationResponse(BaseModel):\n    data: Union[str | None] = None\n    success: bool = True\nclass NormativosResponse(BaseModel):\n    message: Union[str | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "ConversationResponse",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class ConversationResponse(BaseModel):\n    data: Union[str | None] = None\n    success: bool = True\nclass NormativosResponse(BaseModel):\n    message: Union[str | None] = None\n    documents: Union[List | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "NormativosResponse",
        "kind": 6,
        "importPath": "app.models",
        "description": "app.models",
        "peekOfCode": "class NormativosResponse(BaseModel):\n    message: Union[str | None] = None\n    documents: Union[List | None] = None",
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "cors_origins",
        "kind": 5,
        "importPath": "app.utils",
        "description": "app.utils",
        "peekOfCode": "cors_origins = [\n    \"*\"\n]",
        "detail": "app.utils",
        "documentation": {}
    },
    {
        "label": "md_text",
        "kind": 5,
        "importPath": "files.outros2.pymudf_rag",
        "description": "files.outros2.pymudf_rag",
        "peekOfCode": "md_text = pymupdf4llm.to_markdown(\"Pades_CREDIPEU370.949.pdf\")\n# doc = pymupdf.open(\"Pades_CREDIPEU370.949.txt\", filetype=\"txt\")\n# now work with the markdown text, e.g. store as a UTF8-encoded file\npathlib.Path(\"Pades_CREDIPEU370.949.md\").write_bytes(md_text.encode())\n#md_txt = pymupdf4llm.to_markdown(doc)\n# write markdown string to some file\n#pathlib.Path(\"cci.md\").write_bytes(md_txt.encode())\nprint(\"FOI\")\n# https://github.com/search?q=repo%3Adiscourse%2Fdiscourse-ai+markdown&type=code",
        "detail": "files.outros2.pymudf_rag",
        "documentation": {}
    },
    {
        "label": "#md_txt",
        "kind": 5,
        "importPath": "files.outros2.pymudf_rag",
        "description": "files.outros2.pymudf_rag",
        "peekOfCode": "#md_txt = pymupdf4llm.to_markdown(doc)\n# write markdown string to some file\n#pathlib.Path(\"cci.md\").write_bytes(md_txt.encode())\nprint(\"FOI\")\n# https://github.com/search?q=repo%3Adiscourse%2Fdiscourse-ai+markdown&type=code",
        "detail": "files.outros2.pymudf_rag",
        "documentation": {}
    },
    {
        "label": "ChatApp",
        "kind": 6,
        "importPath": "tests.Frontend_NORMAS",
        "description": "tests.Frontend_NORMAS",
        "peekOfCode": "class ChatApp:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"MVP Normas\")\n        self.root.geometry(\"1200x700\")\n        self.main_frame = tk.Frame(root)\n        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n        self.chat_frame = tk.Frame(self.main_frame)\n        self.chat_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n        self.context_frame = tk.Frame(self.main_frame)",
        "detail": "tests.Frontend_NORMAS",
        "documentation": {}
    },
    {
        "label": "data_frame",
        "kind": 5,
        "importPath": "tests.cdelama2",
        "description": "tests.cdelama2",
        "peekOfCode": "data_frame = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\n\"\"\"\nprompt = F'''{data_frame}\\n\\nA varivel `dfs: list[pd.DataFrame]` j est declarada. Sabendo disso responda a pergunta.\\n\\nQual a maior quantidade_vendas?",
        "detail": "tests.cdelama2",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "tests.cdelama2",
        "description": "tests.cdelama2",
        "peekOfCode": "prompt = F'''{data_frame}\\n\\nA varivel `dfs: list[pd.DataFrame]` j est declarada. Sabendo disso responda a pergunta.\\n\\nQual a maior quantidade_vendas?\n    \"\"\" '''\nsuffix = \"\"\"\n    return { \\\"type\\\": \\\"int\\\", \\\"value\\\": result }\n\"\"\"\nfor response in generate(\n  model='codellama:7b-code',\n  prompt=prompt,\n  suffix=suffix,\n  options={",
        "detail": "tests.cdelama2",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "tests.cdelama2",
        "description": "tests.cdelama2",
        "peekOfCode": "suffix = \"\"\"\n    return { \\\"type\\\": \\\"int\\\", \\\"value\\\": result }\n\"\"\"\nfor response in generate(\n  model='codellama:7b-code',\n  prompt=prompt,\n  suffix=suffix,\n  options={\n    'num_predict': 128,\n    'temperature': 0,",
        "detail": "tests.cdelama2",
        "documentation": {}
    },
    {
        "label": "data_frame",
        "kind": 5,
        "importPath": "tests.cdelama3",
        "description": "tests.cdelama3",
        "peekOfCode": "data_frame = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\nReescreva este cdigo:\n```python",
        "detail": "tests.cdelama3",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "tests.cdelama3",
        "description": "tests.cdelama3",
        "peekOfCode": "prompt = F'''{data_frame}\\n\\nA varivel `dfs: list[pd.DataFrame]` j est declarada. Sabendo disso responda a pergunta.\\n\\nQual a maior quantidade_vendas?\n    \"\"\" '''\nsuffix = \"\"\"\n    return result\n\"\"\"\nfor response in generate(\n  model='codellama:7b-code',\n  prompt=data_frame,\n  # suffix=suffix,\n  options={",
        "detail": "tests.cdelama3",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "tests.cdelama3",
        "description": "tests.cdelama3",
        "peekOfCode": "suffix = \"\"\"\n    return result\n\"\"\"\nfor response in generate(\n  model='codellama:7b-code',\n  prompt=data_frame,\n  # suffix=suffix,\n  options={\n    'num_predict': 128,\n    'temperature': 0,",
        "detail": "tests.cdelama3",
        "documentation": {}
    },
    {
        "label": "system_message",
        "kind": 5,
        "importPath": "tests.cdlama",
        "description": "tests.cdlama",
        "peekOfCode": "system_message = \"You are a Data Analyst and pandas expert. Your goal is to help people generate high quality and robust code.\"\ndata_frame = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\n\"\"\"",
        "detail": "tests.cdlama",
        "documentation": {}
    },
    {
        "label": "data_frame",
        "kind": 5,
        "importPath": "tests.cdlama",
        "description": "tests.cdlama",
        "peekOfCode": "data_frame = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\n\"\"\"\nquery = \"\"\"",
        "detail": "tests.cdlama",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "tests.cdlama",
        "description": "tests.cdlama",
        "peekOfCode": "query = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\n```\nUpdate this initial code:",
        "detail": "tests.cdlama",
        "documentation": {}
    },
    {
        "label": "get_session_history",
        "kind": 2,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\nconversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n### Construct retriever ###\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "loader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "docs = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n### Contextualize question ###\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n### Contextualize question ###\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "splits",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "splits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n### Contextualize question ###\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n### Contextualize question ###\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "retriever = vectorstore.as_retriever()\n### Contextualize question ###\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "contextualize_q_system_prompt",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "contextualize_q_prompt",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nhistory_aware_retriever = create_history_aware_retriever(\n    llm, retriever, contextualize_q_prompt\n)",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "history_aware_retriever",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "history_aware_retriever = create_history_aware_retriever(\n    llm, retriever, contextualize_q_prompt\n)\n### Answer question ###\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n{context}\"\"\"\nqa_prompt = ChatPromptTemplate.from_messages(",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "qa_system_prompt",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n{context}\"\"\"\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "qa_prompt",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "qa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\nrag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n### Statefully manage chat history ###",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "question_answer_chain",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\nrag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n### Statefully manage chat history ###\nstore = {}\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\nconversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n### Statefully manage chat history ###\nstore = {}\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\nconversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,\n    get_session_history,",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "store = {}\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\nconversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "conversational_rag_chain",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "conversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",\n)\nfrom langchain_core.messages import HumanMessage\nchat_history = []\nquestion = \"What is Task Decomposition?\"",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "chat_history = []\nquestion = \"What is Task Decomposition?\"\nai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\nchat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\nsecond_question = \"What are common ways of doing it?\"\nai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\nprint(ai_msg_2[\"answer\"])",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "question = \"What is Task Decomposition?\"\nai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\nchat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\nsecond_question = \"What are common ways of doing it?\"\nai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\nprint(ai_msg_2[\"answer\"])",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "ai_msg_1",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\nchat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\nsecond_question = \"What are common ways of doing it?\"\nai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\nprint(ai_msg_2[\"answer\"])",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "second_question",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "second_question = \"What are common ways of doing it?\"\nai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\nprint(ai_msg_2[\"answer\"])",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "ai_msg_2",
        "kind": 5,
        "importPath": "tests.chat_history",
        "description": "tests.chat_history",
        "peekOfCode": "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\nprint(ai_msg_2[\"answer\"])",
        "detail": "tests.chat_history",
        "documentation": {}
    },
    {
        "label": "FilteredRetriever",
        "kind": 6,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "class FilteredRetriever(VectorStoreRetriever):\n    vectorstore: VectorStoreRetriever\n    search_type: str = \"similarity\"\n    search_kwargs: dict = Field(default_factory=dict)\n    filter_prefix: str\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        results = self.vectorstore.get_relevant_documents(query=query)\n        return [doc for doc in results if doc.metadata['source'].startswith(self.filter_prefix)]",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "get_embeddings_by_chunks",
        "kind": 2,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "def get_embeddings_by_chunks(data, chunk_size):\n    chunks = [data[x : x + chunk_size] for x in range(0, len(data), chunk_size)]\n    embeddings_response = [\n        client.embeddings.create(model=model, inputs=c) for c in chunks\n    ]\n    return [d.embedding for e in embeddings_response for d in e.data]\ndf[\"embeddings\"] = get_embeddings_by_chunks(df[\"text\"].tolist(), 50)\ndf.head()\nimport seaborn as sns\nfrom sklearn.manifold import TSNE",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "df = pd.read_csv(\n    \"https://raw.githubusercontent.com/mistralai/cookbook/main/data/Symptom2Disease.csv\",\n    index_col=0,\n)\ndef get_embeddings_by_chunks(data, chunk_size):\n    chunks = [data[x : x + chunk_size] for x in range(0, len(data), chunk_size)]\n    embeddings_response = [\n        client.embeddings.create(model=model, inputs=c) for c in chunks\n    ]\n    return [d.embedding for e in embeddings_response for d in e.data]",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "df[\"embeddings\"]",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "df[\"embeddings\"] = get_embeddings_by_chunks(df[\"text\"].tolist(), 50)\ndf.head()\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport numpy as np\ntsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nimport fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')  # English",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "tsne",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "tsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nimport fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')  # English\nft = fasttext.load_model('cc.en.300.bin')\ndf['fasttext_embeddings'] = df['text'].apply(lambda x: ft.get_word_vector(x).tolist())\ntsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nimport fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')  # English\nft = fasttext.load_model('cc.en.300.bin')\ndf['fasttext_embeddings'] = df['text'].apply(lambda x: ft.get_word_vector(x).tolist())\ntsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nfrom sklearn.model_selection import train_test_split",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "ft",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "ft = fasttext.load_model('cc.en.300.bin')\ndf['fasttext_embeddings'] = df['text'].apply(lambda x: ft.get_word_vector(x).tolist())\ntsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Create a train / test split\ntrain_x, test_x, train_y, test_y = train_test_split(",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "df['fasttext_embeddings']",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "df['fasttext_embeddings'] = df['text'].apply(lambda x: ft.get_word_vector(x).tolist())\ntsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Create a train / test split\ntrain_x, test_x, train_y, test_y = train_test_split(\n    df[\"embeddings\"], df[\"label\"], test_size=0.2",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "tsne",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "tsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\nax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Create a train / test split\ntrain_x, test_x, train_y, test_y = train_test_split(\n    df[\"embeddings\"], df[\"label\"], test_size=0.2\n)",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\nsns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Create a train / test split\ntrain_x, test_x, train_y, test_y = train_test_split(\n    df[\"embeddings\"], df[\"label\"], test_size=0.2\n)\n# Normalize features",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "scaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x.to_list())\ntest_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "train_x",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "train_x = scaler.fit_transform(train_x.to_list())\ntest_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "test_x",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "test_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")\n# Classify a single example",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "clf",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")\n# Classify a single example\ntext = \"I've been experiencing frequent headaches and vision problems.\"\nclf.predict([get_text_embedding([text])])\n# Create a train / test split",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "text = \"I've been experiencing frequent headaches and vision problems.\"\nclf.predict([get_text_embedding([text])])\n# Create a train / test split\ntrain_x, test_x, train_y, test_y = train_test_split(\n    df[\"fasttext_embeddings\"], df[\"label\"], test_size=0.2\n)\n# Normalize features\nscaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x.to_list())\ntest_x = scaler.transform(test_x.to_list())",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "scaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x.to_list())\ntest_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "train_x",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "train_x = scaler.fit_transform(train_x.to_list())\ntest_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "test_x",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "test_x = scaler.transform(test_x.to_list())\n# Train a classifier and compute the test accuracy\n# For a real problem, C should be properly cross validated and the confusion matrix analyzed\nclf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")\nfrom sklearn.cluster import KMeans",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "clf",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(\n    train_x, train_y.to_list()\n)\n# you can also try the sag algorithm:\n# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\nprint(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=24, max_iter=1000)\nmodel.fit(df['embeddings'].to_list())\ndf[\"cluster\"] = model.labels_",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "model = KMeans(n_clusters=24, max_iter=1000)\nmodel.fit(df['embeddings'].to_list())\ndf[\"cluster\"] = model.labels_\nprint(*df[df.cluster==23].text.head(3), sep='\\n\\n')\nfiltered_retriever = FilteredRetriever(vectorstore=store.as_retriever(), filter_prefix=source_filter)\nchain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=filtered_retriever,\n    memory=st.session_state.memory,\n    verbose=True,",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "df[\"cluster\"]",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "df[\"cluster\"] = model.labels_\nprint(*df[df.cluster==23].text.head(3), sep='\\n\\n')\nfiltered_retriever = FilteredRetriever(vectorstore=store.as_retriever(), filter_prefix=source_filter)\nchain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=filtered_retriever,\n    memory=st.session_state.memory,\n    verbose=True,\n    return_source_documents=True,\n)",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "filtered_retriever",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "filtered_retriever = FilteredRetriever(vectorstore=store.as_retriever(), filter_prefix=source_filter)\nchain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=filtered_retriever,\n    memory=st.session_state.memory,\n    verbose=True,\n    return_source_documents=True,\n)\nclass FilteredRetriever(VectorStoreRetriever):\n    vectorstore: VectorStoreRetriever",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "tests.chunks",
        "description": "tests.chunks",
        "peekOfCode": "chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=filtered_retriever,\n    memory=st.session_state.memory,\n    verbose=True,\n    return_source_documents=True,\n)\nclass FilteredRetriever(VectorStoreRetriever):\n    vectorstore: VectorStoreRetriever\n    search_type: str = \"similarity\"",
        "detail": "tests.chunks",
        "documentation": {}
    },
    {
        "label": "correct_spelling_errors",
        "kind": 2,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "def correct_spelling_errors(text):\n    # Define dictionary of common spelling mistakes and their corrections\n    spelling_corrections = {\n         oherence : everything,\n         oherence : refinement,\n        accurte: accurate,\n        retrievel: retrieval,\n         oherence : correcting,\n        refning: refining,\n         oherenc: enhances,",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "response_test",
        "kind": 2,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "def response_test(question:str, context:str, model:str = \"gpt-4\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": MESSAGE_SYSTEM_CONTENT,\n            },\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": context},",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "text = \"I love coding!  #PythonProgramming is fun!  Lets clean some text \"\n# Tokenization\ntokens = word_tokenize(text)\n# Remove Noise\ncleaned_tokens = [re.sub(r'[^\\w\\s], ', token) for token in tokens]\n# Normalization (convert to lowercase)\ncleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words(\"portuguese\"))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "tokens = word_tokenize(text)\n# Remove Noise\ncleaned_tokens = [re.sub(r'[^\\w\\s], ', token) for token in tokens]\n# Normalization (convert to lowercase)\ncleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words(\"portuguese\"))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [re.sub(r'[^\\w\\s], ', token) for token in tokens]\n# Normalization (convert to lowercase)\ncleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words(\"portuguese\"))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words(\"portuguese\"))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\n# output:\n# [love, coding, pythonprogramming, fun, clean, text]",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "stop_words = set(stopwords.words(\"portuguese\"))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\n# output:\n# [love, coding, pythonprogramming, fun, clean, text]\nimport re\n# Sample text with spelling errors",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\n# output:\n# [love, coding, pythonprogramming, fun, clean, text]\nimport re\n# Sample text with spelling errors\ntext_with_errors = \"\"\"But s not  oherence  about more language  oherence . ",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "lemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\n# output:\n# [love, coding, pythonprogramming, fun, clean, text]\nimport re\n# Sample text with spelling errors\ntext_with_errors = \"\"\"But s not  oherence  about more language  oherence . \nOther important aspect is ensuring accurte retrievel by  oherence  product name spellings. \nAdditionally, refning descriptions  oherenc the  oherence of the contnt.\"\"\"",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\n# output:\n# [love, coding, pythonprogramming, fun, clean, text]\nimport re\n# Sample text with spelling errors\ntext_with_errors = \"\"\"But s not  oherence  about more language  oherence . \nOther important aspect is ensuring accurte retrievel by  oherence  product name spellings. \nAdditionally, refning descriptions  oherenc the  oherence of the contnt.\"\"\"\n# Function to correct spelling errors",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "text_with_errors",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "text_with_errors = \"\"\"But s not  oherence  about more language  oherence . \nOther important aspect is ensuring accurte retrievel by  oherence  product name spellings. \nAdditionally, refning descriptions  oherenc the  oherence of the contnt.\"\"\"\n# Function to correct spelling errors\ndef correct_spelling_errors(text):\n    # Define dictionary of common spelling mistakes and their corrections\n    spelling_corrections = {\n         oherence : everything,\n         oherence : refinement,\n        accurte: accurate,",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_text",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_text = correct_spelling_errors(text_with_errors)\nprint(cleaned_text)\n# output\n# But its not everything about more language refinement.\n# other important aspect is ensuring accurate retrieval by correcting product name spellings.\n# Additionally, refining descriptions enhances the coherence of the content.\nimport spacy\nimport json\n# Load English language model\nnlp = spacy.load(\"en_core_web_sm\")",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "nlp = spacy.load(\"en_core_web_sm\")\n# Sample text with meta data candidates\ntext = \"\"\"In a blog post titled The Top 10 Tech Trends of 2024, \nJohn Doe discusses the rise of artificial intelligence and machine learning \nin various industries. The article mentions companies like Google and Microsoft \nas pioneers in AI research. Additionally, it highlights emerging technologies \nsuch as natural language processing and computer vision.\"\"\"\n# Process the text with spaCy\ndoc = nlp(text)\n# Extract named entities and their labels",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "text = \"\"\"In a blog post titled The Top 10 Tech Trends of 2024, \nJohn Doe discusses the rise of artificial intelligence and machine learning \nin various industries. The article mentions companies like Google and Microsoft \nas pioneers in AI research. Additionally, it highlights emerging technologies \nsuch as natural language processing and computer vision.\"\"\"\n# Process the text with spaCy\ndoc = nlp(text)\n# Extract named entities and their labels\nmeta_data = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n# Convert meta data to JSON format",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "doc = nlp(text)\n# Extract named entities and their labels\nmeta_data = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n# Convert meta data to JSON format\nmeta_data_json = json.dumps(meta_data)\nprint(meta_data_json)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n# Sample documents\ndocuments = [",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "meta_data",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "meta_data = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n# Convert meta data to JSON format\nmeta_data_json = json.dumps(meta_data)\nprint(meta_data_json)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n# Sample documents\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Natural language processing involves analyzing and understanding human languages.\",",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "meta_data_json",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "meta_data_json = json.dumps(meta_data)\nprint(meta_data_json)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n# Sample documents\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Natural language processing involves analyzing and understanding human languages.\",\n    \"Deep learning algorithms mimic the structure and function of the human brain.\",\n    \"Sentiment analysis aims to determine the emotional tone of a text.\"",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "documents = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Natural language processing involves analyzing and understanding human languages.\",\n    \"Deep learning algorithms mimic the structure and function of the human brain.\",\n    \"Sentiment analysis aims to determine the emotional tone of a text.\"\n]\n# Convert text into numerical feature vectors\nvectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n# Apply Latent Dirichlet Allocation (LDA) for topic modeling",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "vectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n# Apply Latent Dirichlet Allocation (LDA) for topic modeling\nlda = LatentDirichletAllocation(n_components=2, random_state=42)\nlda.fit(X)\n# Display topics\nfor topic_idx, topic in enumerate(lda.components_):\n    print(\"Topic %d:\" % (topic_idx + 1))\n    print(\" \".join([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-5 - 1:-1]]))\n# output",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "X = vectorizer.fit_transform(documents)\n# Apply Latent Dirichlet Allocation (LDA) for topic modeling\nlda = LatentDirichletAllocation(n_components=2, random_state=42)\nlda.fit(X)\n# Display topics\nfor topic_idx, topic in enumerate(lda.components_):\n    print(\"Topic %d:\" % (topic_idx + 1))\n    print(\" \".join([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-5 - 1:-1]]))\n# output\n#",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "lda",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "lda = LatentDirichletAllocation(n_components=2, random_state=42)\nlda.fit(X)\n# Display topics\nfor topic_idx, topic in enumerate(lda.components_):\n    print(\"Topic %d:\" % (topic_idx + 1))\n    print(\" \".join([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-5 - 1:-1]]))\n# output\n#\n#Topic 1:\n#learning machine subset artificial intelligence",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "synthetic_text",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "synthetic_text = \"\"\"\nSarah (S): Technology Enthusiast\nMark (M): AI Expert\nS: Hey Mark! How's it going? Heard about the latest advancements in Generative AI (GA)?\nM: Hey Sarah! Yes, I've been diving deep into the realm of GA lately. It's fascinating how it's shaping the future of technology!\nS: Absolutely! I mean, GA has been making waves across various industries. What do you think is driving its significance?\nM: Well, GA, especially Retrieval Augmented Generative (RAG), is revolutionizing content generation. It's not just about regurgitating information anymore; it's about creating contextually relevant and engaging content.\nS: Right! And with Machine Learning (ML) becoming more sophisticated, the possibilities seem endless.\nM: Exactly! With advancements in ML algorithms like GPT (Generative Pre-trained Transformer), we're seeing unprecedented levels of creativity in AI-generated content.\nS: But what about concerns regarding bias and ethics in GA?",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "tokens = word_tokenize(synthetic_text)\n# Remove Noise\ncleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n# Normalization (convert to lowercase)\ncleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words('portuguese'))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n# Normalization (convert to lowercase)\ncleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words('portuguese'))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [token.lower() for token in cleaned_tokens]\n# Remove Stopwords\nstop_words = set(stopwords.words('portuguese'))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\nMESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "stop_words = set(stopwords.words('portuguese'))\ncleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\nMESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the\nprovided context below. \nMake sure not to make any changes to the context if possible,",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\nMESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the\nprovided context below. \nMake sure not to make any changes to the context if possible,\nwhen prepare answers so as to provide accurate responses. If the answer ",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "lemmatizer = WordNetLemmatizer()\ncleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\nMESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the\nprovided context below. \nMake sure not to make any changes to the context if possible,\nwhen prepare answers so as to provide accurate responses. If the answer \ncannot be found in context, just politely say that you do not know, \ndo not try to make up an answer.\"\"\"",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "cleaned_tokens",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\nprint(cleaned_tokens)\nMESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the\nprovided context below. \nMake sure not to make any changes to the context if possible,\nwhen prepare answers so as to provide accurate responses. If the answer \ncannot be found in context, just politely say that you do not know, \ndo not try to make up an answer.\"\"\"\ndef response_test(question:str, context:str, model:str = \"gpt-4\"):",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "MESSAGE_SYSTEM_CONTENT",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "MESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps \na customer with answering questions. Please answer the question based on the\nprovided context below. \nMake sure not to make any changes to the context if possible,\nwhen prepare answers so as to provide accurate responses. If the answer \ncannot be found in context, just politely say that you do not know, \ndo not try to make up an answer.\"\"\"\ndef response_test(question:str, context:str, model:str = \"gpt-4\"):\n    response = client.chat.completions.create(\n        model=model,",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "question1",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "question1 = \"What are some specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models?\"\nresponse = response_test(question1, synthetic_text)\nprint(response)\n#Output\n# I'm sorry, but the context provided doesn't contain specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models.\nresponse = response_test(question1, new_content_string)\nprint(response)\n#Output:\n# The context mentions Adversarial Training (AT) as a technique that can \n# help mitigate biases in Generative AI models. However, it does not provide ",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "response = response_test(question1, synthetic_text)\nprint(response)\n#Output\n# I'm sorry, but the context provided doesn't contain specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models.\nresponse = response_test(question1, new_content_string)\nprint(response)\n#Output:\n# The context mentions Adversarial Training (AT) as a technique that can \n# help mitigate biases in Generative AI models. However, it does not provide \n#any specific techniques within Adversarial Training itself.",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "tests.cleaning",
        "description": "tests.cleaning",
        "peekOfCode": "response = response_test(question1, new_content_string)\nprint(response)\n#Output:\n# The context mentions Adversarial Training (AT) as a technique that can \n# help mitigate biases in Generative AI models. However, it does not provide \n#any specific techniques within Adversarial Training itself.",
        "detail": "tests.cleaning",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.comprovante",
        "description": "tests.comprovante",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\nembeddings    = OllamaEmbeddings(model=EMBD)\nllm           = ChatOllama(\n    model=MODEL_Q2,\n    keep_alive='1h',\n    temperature=0.3,\n    top_k=50\n)\nllm_query       = ChatOllama(\n    model       = MODEL_Q2,",
        "detail": "tests.comprovante",
        "documentation": {}
    },
    {
        "label": "pytesseract.pytesseract.tesseract_cmd",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\n# Read the image from which text needs to be extracted\nimg = cv2.imread('./files/to_img/cci1010_0OTSU.png')\n# Convert the image to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# Performing OTSU threshold\nret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n# dilation parameter, bigger means less rectangle\nrect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n# Applying dilation on the threshold image",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "img = cv2.imread('./files/to_img/cci1010_0OTSU.png')\n# Convert the image to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# Performing OTSU threshold\nret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n# dilation parameter, bigger means less rectangle\nrect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n# Applying dilation on the threshold image\ndilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n# Finding contours",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "gray",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# Performing OTSU threshold\nret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n# dilation parameter, bigger means less rectangle\nrect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n# Applying dilation on the threshold image\ndilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n# Finding contours\ncontours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n# Creating a copy of image",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "rect_kernel",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n# Applying dilation on the threshold image\ndilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n# Finding contours\ncontours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n# Creating a copy of image\nim2 = gray.copy()\ncnt_list=[]\nfor cnt in contours:\n    x, y, w, h = cv2.boundingRect(cnt)",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "dilation",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n# Finding contours\ncontours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n# Creating a copy of image\nim2 = gray.copy()\ncnt_list=[]\nfor cnt in contours:\n    x, y, w, h = cv2.boundingRect(cnt)\n    # Drawing a rectangle on the copied image\n    rect = cv2.rectangle(im2, (x, y), (x + w, y + h), (0, 255, 0), 5)",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "im2",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "im2 = gray.copy()\ncnt_list=[]\nfor cnt in contours:\n    x, y, w, h = cv2.boundingRect(cnt)\n    # Drawing a rectangle on the copied image\n    rect = cv2.rectangle(im2, (x, y), (x + w, y + h), (0, 255, 0), 5)\n    cv2.circle(im2,(x,y),8,(255,255,0),8)\n    # Cropping the text block for giving input to OCR\n    cropped = im2[y:y + h, x:x + w]\n    # Apply OCR on the cropped image",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "sorted_list",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "sorted_list = sorted(cnt_list, key=lambda x: x[1])\n# A text file is created \nfile = open(\"recognized.txt\", \"w+\")\nfile.write(\"\")\nfile.close()\nfor x,y,text in sorted_list:\n    # Open the file in append mode\n    file = open(\"recognized.txt\", \"a\")\n    # Appending the text into the file\n    file.write(text)",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "file",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "file = open(\"recognized.txt\", \"w+\")\nfile.write(\"\")\nfile.close()\nfor x,y,text in sorted_list:\n    # Open the file in append mode\n    file = open(\"recognized.txt\", \"a\")\n    # Appending the text into the file\n    file.write(text)\n    file.write(\"\\n\")\n    # Close the file",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "rgb_image",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "rgb_image = cv2.resize(im2, (0, 0), fx = 0.4, fy = 0.4)\ndilation = cv2.resize(dilation, (0, 0), fx = 0.4, fy = 0.4)\n#thresh1 = cv2.resize(thresh1, (0, 0), fx = 0.4, fy = 0.4)\n# show the image, provide the window name first\n#cv2.imshow('thresh1', thresh1)\ncv2.imshow('dilation', dilation)\ncv2.imshow('gray', gray)\n# add wait key. window waits until the user presses a key\ncv2.waitKey(0)\n# and finally destroy/close all open windows",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "dilation",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "dilation = cv2.resize(dilation, (0, 0), fx = 0.4, fy = 0.4)\n#thresh1 = cv2.resize(thresh1, (0, 0), fx = 0.4, fy = 0.4)\n# show the image, provide the window name first\n#cv2.imshow('thresh1', thresh1)\ncv2.imshow('dilation', dilation)\ncv2.imshow('gray', gray)\n# add wait key. window waits until the user presses a key\ncv2.waitKey(0)\n# and finally destroy/close all open windows\ncv2.destroyAllWindows()",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "#thresh1",
        "kind": 5,
        "importPath": "tests.dilatacao",
        "description": "tests.dilatacao",
        "peekOfCode": "#thresh1 = cv2.resize(thresh1, (0, 0), fx = 0.4, fy = 0.4)\n# show the image, provide the window name first\n#cv2.imshow('thresh1', thresh1)\ncv2.imshow('dilation', dilation)\ncv2.imshow('gray', gray)\n# add wait key. window waits until the user presses a key\ncv2.waitKey(0)\n# and finally destroy/close all open windows\ncv2.destroyAllWindows()",
        "detail": "tests.dilatacao",
        "documentation": {}
    },
    {
        "label": "Extract_context",
        "kind": 2,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "def Extract_context(query):\n    chroma_client = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    db = Chroma(\n        client=chroma_client,\n        collection_name=\"my_collection\",\n        embedding_function=embedding_function,\n    )\n    docs = db.similarity_search(query)\n    fullcontent =''",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "get_system_message_rag",
        "kind": 2,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "def get_system_message_rag(content):\n        return f\"\"\"You are an expert consultant helping executive advisors to get relevant information from internal documents.\n        Generate your response by following the steps below:\n        1. Recursively break down the question into smaller questions.\n        2. For each question/directive:\n            2a. Select the most relevant information from the context in light of the conversation history.\n        3. Generate a draft response using selected information.\n        4. Remove duplicate content from draft response.\n        5. Generate your final response after adjusting it to increase accuracy and relevance.\n        6. Do not try to summarise the answers, explain it properly.",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "get_ques_response_prompt",
        "kind": 2,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "def get_ques_response_prompt(question):\n    return f\"\"\"\n    ==============================================================\n    Based on the above context, please provide the answer to the following question:\n    {question}\n    \"\"\"\nfrom ollama import Client\ndef generate_rag_response(content,question):\n    client = Client(host='http://host.docker.internal:11434')\n    stream = client.chat(model='mistral', messages=[",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "generate_rag_response",
        "kind": 2,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "def generate_rag_response(content,question):\n    client = Client(host='http://host.docker.internal:11434')\n    stream = client.chat(model='mistral', messages=[\n    {\"role\": \"system\", \"content\": get_system_message_rag(content)},            \n    {\"role\": \"user\", \"content\": get_ques_response_prompt(question)}\n    ],stream=True)\n    print(get_system_message_rag(content))\n    print(get_ques_response_prompt(question))\n    print(\"####### THINKING OF ANSWER............ \")\n    full_answer = ''",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "respond_to_query",
        "kind": 2,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "def respond_to_query():\n    if request.method == 'POST':\n        data = request.get_json()\n        # Assuming the query is sent as a JSON object with a key named 'query'\n        query = data.get('query')\n        # Here you can process the query and generate a response\n        response = f'This is the response to your query:\\n {get_reponse(query)}'\n        return response\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0')",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "loader = PyPDFLoader(\"2404.07143.pdf\")\npages = loader.load_and_split()\n# split it into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\n# create the open-source embedding function\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# create the chroma client\nimport uuid\nimport chromadb",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "pages",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "pages = loader.load_and_split()\n# split it into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\n# create the open-source embedding function\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# create the chroma client\nimport uuid\nimport chromadb\nfrom chromadb.config import Settings",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\n# create the open-source embedding function\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# create the chroma client\nimport uuid\nimport chromadb\nfrom chromadb.config import Settings\nclient = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))\nclient.reset()  # resets the database",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "docs = text_splitter.split_documents(pages)\n# create the open-source embedding function\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# create the chroma client\nimport uuid\nimport chromadb\nfrom chromadb.config import Settings\nclient = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))\nclient.reset()  # resets the database\ncollection = client.create_collection(\"my_collection\")",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "embedding_function",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# create the chroma client\nimport uuid\nimport chromadb\nfrom chromadb.config import Settings\nclient = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))\nclient.reset()  # resets the database\ncollection = client.create_collection(\"my_collection\")\nfor doc in docs:\n    collection.add(",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "client = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))\nclient.reset()  # resets the database\ncollection = client.create_collection(\"my_collection\")\nfor doc in docs:\n    collection.add(\n        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n    )\n# tell LangChain to use our client and collection name\ndb = Chroma(\n    client=client,",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "collection = client.create_collection(\"my_collection\")\nfor doc in docs:\n    collection.add(\n        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n    )\n# tell LangChain to use our client and collection name\ndb = Chroma(\n    client=client,\n    collection_name=\"my_collection\",\n    embedding_function=embedding_function,",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "db = Chroma(\n    client=client,\n    collection_name=\"my_collection\",\n    embedding_function=embedding_function,\n)\nquery = \"What training does the model have?\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nimport chromadb\nfrom chromadb.config import Settings",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "query = \"What training does the model have?\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings.sentence_transformer import (\n    SentenceTransformerEmbeddings,\n)\ndef Extract_context(query):",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "tests.docker",
        "description": "tests.docker",
        "peekOfCode": "docs = db.similarity_search(query)\nprint(docs[0].page_content)\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings.sentence_transformer import (\n    SentenceTransformerEmbeddings,\n)\ndef Extract_context(query):\n    chroma_client = chromadb.HttpClient(host='host.docker.internal', port=8000,settings=Settings(allow_reset=True))",
        "detail": "tests.docker",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "tests.eocr",
        "description": "tests.eocr",
        "peekOfCode": "reader = easyocr.Reader(['pt','en'])\nresult = reader.readtext('./files/to_img/CCI1.081_2024_0_GRAY.png', detail=0, paragraph=True, text_threshold=0.2)\nprint(result)",
        "detail": "tests.eocr",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "tests.eocr",
        "description": "tests.eocr",
        "peekOfCode": "result = reader.readtext('./files/to_img/CCI1.081_2024_0_GRAY.png', detail=0, paragraph=True, text_threshold=0.2)\nprint(result)",
        "detail": "tests.eocr",
        "documentation": {}
    },
    {
        "label": "load_pdf",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def load_pdf(file_path: str, text_splitter: CharacterTextSplitter, extract_images=True) -> List[Document]:\n    loader = PyPDFLoader(file_path, extract_images=extract_images)\n    return loader.load_and_split(text_splitter)\ndef load_docx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredWordDocumentLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef load_xlsx2():\n    print('1')\n    # excel2img.export_img(\"./files/AnaliseCCF.xlsx\",\"AnaliseCCFTratamentos.png\",\"Tratamentos\")\ndef load_xlsx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "load_docx",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def load_docx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredWordDocumentLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef load_xlsx2():\n    print('1')\n    # excel2img.export_img(\"./files/AnaliseCCF.xlsx\",\"AnaliseCCFTratamentos.png\",\"Tratamentos\")\ndef load_xlsx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredExcelLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef extrair_xlsx() -> None:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "load_xlsx2",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def load_xlsx2():\n    print('1')\n    # excel2img.export_img(\"./files/AnaliseCCF.xlsx\",\"AnaliseCCFTratamentos.png\",\"Tratamentos\")\ndef load_xlsx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredExcelLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef extrair_xlsx() -> None:\n    text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n    docs = load_xlsx(excel_file_path, text_splitter)\n    # print(docs)",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "load_xlsx",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def load_xlsx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredExcelLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef extrair_xlsx() -> None:\n    text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n    docs = load_xlsx(excel_file_path, text_splitter)\n    # print(docs)\n    # grava somente as tabelas\n    for doc in docs:\n        with open(F\"{doc.metadata['page_name'].replace(' ', '_')}_{doc.metadata['page_number']}.html\", 'w', encoding='UTF-8') as file:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "extrair_xlsx",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def extrair_xlsx() -> None:\n    text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n    docs = load_xlsx(excel_file_path, text_splitter)\n    # print(docs)\n    # grava somente as tabelas\n    for doc in docs:\n        with open(F\"{doc.metadata['page_name'].replace(' ', '_')}_{doc.metadata['page_number']}.html\", 'w', encoding='UTF-8') as file:\n            if 'text_as_html' in doc.metadata.keys():\n                file.write(doc.metadata['text_as_html'])\n            # else:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "extrair_docs",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def extrair_docs() -> None:\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = load_docx(word_file_path, text_splitter)\n    print(docs[0])\n    with open('AtivoProblematico.html', 'w', encoding='UTF-8') as file:\n        for doc in docs:\n            if 'text_as_html' in doc.metadata.keys():\n                file.write(doc.metadata['text_as_html'])\n            else:\n                file.write(doc.page_content)",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "extrair_pdfs",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def extrair_pdfs() -> None:\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    docs = load_pdf(pdf_file_path, text_splitter, True)\n    with open('comprovante_com_ocr.txt', 'w', encoding='UTF-8') as file:\n        for doc in docs:\n            file.write(doc.page_content)\n    docs = load_pdf(pdf_file_path, text_splitter, False)\n    with open('comprovante_sem_ocr.txt', 'w', encoding='UTF-8') as file:\n        for doc in docs:\n            file.write(doc.page_content)",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "chat_pandas_ai",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def chat_pandas_ai():\n    # Get-Process ollama* | Sort-Object -Property CPU -Descending | Select-Object -First 10\n    llm = Ollama(model=\"gemma2:2b-instruct-q4_K_M\", temperature= 0, top_k=20, top_p=4, keep_alive=0, num_predict=1234477)\n    data_list = pd.read_html('Caso_PF_2.html', header=0, encoding=\"utf-8\") # essa porra vem com nmeros nas colunas\n    data_frame = SmartDataframe(df=data_list[0], config={\"llm\": llm, \"custom_whitelisted_dependencies\": [\"any_module\"], \"encoding\": \"utf-8\", \"verbose\": True, \"enforce_privacy\": True, \"is_conversational_answer\": False, \"enable_cache\": False}, name=\"Casos Pessoa Fsica\")\n    output = data_frame.chat(query=\"What is the minimum, maximum and range CCF?\")\n    print(f\"Resposta PANDAS AI com OLLAMA:\\n{output}\")\n    # import sys; sys.exit(0)\n# This is a utility method to convert message to prompt that are understood by\n# the Llama 2 model",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "build_llama2_prompt",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def build_llama2_prompt(messages):\n    start_prompt = \"<s>[INST] \"\n    end_prompt = \" [/INST]\"\n    conversation = []\n    for index, message in enumerate(messages):\n        if message[\"role\"] == \"system\" and index == 0:\n            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n        elif message[\"role\"] == \"user\":\n            conversation.append(message[\"content\"].strip())\n        else:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "chat_pandas_ai_excel",
        "kind": 2,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "def chat_pandas_ai_excel():\n<<<<<<< HEAD\n    llm = Ollama(model=\"gemma2:2b-instruct-q4_K_M\", temperature=0, top_k=20, top_p=4, keep_alive='1h', num_predict=1234477)\n    data_frame = SmartDataframe(df=\"../files/outros/analise_excel.xlsx\", config={\"llm\": llm, \"custom_whitelisted_dependencies\": [\"any_module\"], \"encoding\": \"utf-8\", \"verbose\": True, \"enforce_privacy\": True, \"is_conversational_answer\": False, \"enable_cache\": False}, name=\"Casos Pessoa Fsica\")\n    output = data_frame.chat(query=\"Witch is the minimum and maximum \\\"quantidade_vendas\\\" in the dataset?\")\n=======\n    llm = Ollama(\n        model=\"codellama:7b-code\", \n        temperature= 0, \n        top_k=40,",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "db = None\npdf_file_path = (\n    \"./files/comprovante.pdf\"\n)\nword_file_path = (\n    \"./files/AtivoProblematico.docx\"\n)\nexcel_file_path = (\n    \"../files/outros/analise_excel.xlsx\"\n)",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "pdf_file_path",
        "kind": 5,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "pdf_file_path = (\n    \"./files/comprovante.pdf\"\n)\nword_file_path = (\n    \"./files/AtivoProblematico.docx\"\n)\nexcel_file_path = (\n    \"../files/outros/analise_excel.xlsx\"\n)\ndef load_pdf(file_path: str, text_splitter: CharacterTextSplitter, extract_images=True) -> List[Document]:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "word_file_path",
        "kind": 5,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "word_file_path = (\n    \"./files/AtivoProblematico.docx\"\n)\nexcel_file_path = (\n    \"../files/outros/analise_excel.xlsx\"\n)\ndef load_pdf(file_path: str, text_splitter: CharacterTextSplitter, extract_images=True) -> List[Document]:\n    loader = PyPDFLoader(file_path, extract_images=extract_images)\n    return loader.load_and_split(text_splitter)\ndef load_docx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "excel_file_path",
        "kind": 5,
        "importPath": "tests.extrator_utils",
        "description": "tests.extrator_utils",
        "peekOfCode": "excel_file_path = (\n    \"../files/outros/analise_excel.xlsx\"\n)\ndef load_pdf(file_path: str, text_splitter: CharacterTextSplitter, extract_images=True) -> List[Document]:\n    loader = PyPDFLoader(file_path, extract_images=extract_images)\n    return loader.load_and_split(text_splitter)\ndef load_docx(file_path: str, text_splitter: CharacterTextSplitter) -> List[Document]:\n    loader = UnstructuredWordDocumentLoader(file_path, mode=\"elements\")\n    return loader.load_and_split(text_splitter)\ndef load_xlsx2():",
        "detail": "tests.extrator_utils",
        "documentation": {}
    },
    {
        "label": "ChatApp",
        "kind": 6,
        "importPath": "tests.frontend_NORMASG",
        "description": "tests.frontend_NORMASG",
        "peekOfCode": "class ChatApp:\n    def __init__(self):\n        self.context_text = \"\"\n        self.model_var = \"llama3\"\n        self.caching_var = True\n        self.max_doc_size_var = 10000\n    def send_message(self, message):\n        if message:\n            self.display_message(\"Voc: \" + message, \"user\")\n            threading.Thread(target=self.query_api, args=(message,)).start()",
        "detail": "tests.frontend_NORMASG",
        "documentation": {}
    },
    {
        "label": "build_llama_prompt",
        "kind": 2,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "def build_llama_prompt(data_frame: str) -> str:\n  return F\"\"\"\n<dataframe>\n{data_frame}\n</dataframe>\n\"\"\"\ndf = pd.read_excel(\n  \"./files/outros/intercredis_remessa_expressa.xlsx\",\n  header=None,\n  index_col=False,",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "system_message",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "system_message = \"Voc  um analista de dados e especialista em pandas que escreve em portugus. Seu objetivo  ajudar as pessoas a gerar cdigo robusto e de alta qualidade.\"\nsystem_message_excel = \"Voc  um dedicado analista de dados, especialista em tabelas, excel, relatrios e planilhas. Seu objetivo  ajudar as pessoas fazendo anlises detalhadas, insights robustos e sugestes de anlises especficas para informaes relevantes sobre os dados do dataframe.\"\ndef build_llama_prompt(data_frame: str) -> str:\n  return F\"\"\"\n<dataframe>\n{data_frame}\n</dataframe>\n\"\"\"\ndf = pd.read_excel(\n  \"./files/outros/intercredis_remessa_expressa.xlsx\",",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "system_message_excel",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "system_message_excel = \"Voc  um dedicado analista de dados, especialista em tabelas, excel, relatrios e planilhas. Seu objetivo  ajudar as pessoas fazendo anlises detalhadas, insights robustos e sugestes de anlises especficas para informaes relevantes sobre os dados do dataframe.\"\ndef build_llama_prompt(data_frame: str) -> str:\n  return F\"\"\"\n<dataframe>\n{data_frame}\n</dataframe>\n\"\"\"\ndf = pd.read_excel(\n  \"./files/outros/intercredis_remessa_expressa.xlsx\",\n  header=None,",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "df = pd.read_excel(\n  \"./files/outros/intercredis_remessa_expressa.xlsx\",\n  header=None,\n  index_col=False,\n  keep_default_na=False,\n  sheet_name=[0, 1, 2, 3, 4, 5, 6, 7],\n  date_format=\"DD/MM/YYYY HH:MM:SS\",\n  parse_dates=True,\n  #na_values=['-'],\n  verbose=False,",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "query = F\"\"\"\n{build_llama_prompt(data_frame=df[6].to_csv(index=False, encoding=\"UTF-8\", sep=\";\"))}\nVerifique a estrutura do dataframe, altere o que for necessrio para torna-la mais legvel, remova textos como \\\"Unnamed\\\", \\\"null\\\" e informaes que paream ser marcaes de dados vazios ou nulos.\nEscreva SOMENTE o dataframe reestruturado no formato MARKDOWN e verifique se o mesmo est legvel.\nPor fim retorne APENAS a anlise e insighs detalhados sobre o dataframe. Responda em portugus.\n\"\"\"\nquery = F\"\"\"\n{build_llama_prompt(data_frame=df[6].to_csv(index=False, encoding=\"UTF-8\"))}\nCom base no conjunto de dados fornecido, analise e apresente de 3 a 5 observaes, destaques ou tendncias mais interessantes.\nIsto pode incluir a identificao de segmentos (por exemplo, datas, valores, empresas, taxas, reclamaes, etc.) com maior probabilidade de responder de uma determinada forma,",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "query = F\"\"\"\n{build_llama_prompt(data_frame=df[6].to_csv(index=False, encoding=\"UTF-8\"))}\nCom base no conjunto de dados fornecido, analise e apresente de 3 a 5 observaes, destaques ou tendncias mais interessantes.\nIsto pode incluir a identificao de segmentos (por exemplo, datas, valores, empresas, taxas, reclamaes, etc.) com maior probabilidade de responder de uma determinada forma,\npadres significativos ou insights inesperados dos dados.\nSua anlise deve ser detalhada e criteriosa, concentrando-se nos aspectos mais atraentes do conjunto de dados.\nFornea um resumo claro e conciso que destaque as principais concluses, garantindo que as tendncias ou observaes sejam apresentadas de forma envolvente e informativa.\nCertifique-se de que sua resposta incentive a criatividade e a originalidade na identificao e apresentao dos insights mais atraentes do conjunto de dados, mantendo a preciso e a relevncia.\nResponda em portugus.\n\"\"\"",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "data_frame",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "data_frame = \"\"\"\n<dataframe>\ndfs[0]:4x3\nproduto,preco,quantidade_vendas\nFrutas,8.0,5\nSalgado,10.0,10\nBolo,25.0,50\n</dataframe>\n\"\"\"\nquery = F\"\"\"",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "tests.lama3",
        "description": "tests.lama3",
        "peekOfCode": "query = F\"\"\"\n{data_frame}\\n\\n\\nFaa uma anlise do dataframe. Explique a sua resposta em portugus.\n\"\"\"\nprint(query)\nfor part in ollama.generate(\n    model='llama3:8b-instruct-q2_K',\n    messages=[\n      {'role': 'system', 'content': system_message_excel},\n      {'role': 'user', 'content': query}\n    ],",
        "detail": "tests.lama3",
        "documentation": {}
    },
    {
        "label": "convert_to_base64",
        "kind": 2,
        "importPath": "tests.lc_image",
        "description": "tests.lc_image",
        "peekOfCode": "def convert_to_base64(pil_image: Image):\n    buffered = BytesIO()\n    pil_image.save(buffered, format=\"PNG\")\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return img_str\ndef load_image(image_path: str):\n    pil_image = Image.open(image_path)\n    image_b64 = convert_to_base64(pil_image)\n    print(\"Loaded image successfully!\")\n    return image_b64",
        "detail": "tests.lc_image",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "tests.lc_image",
        "description": "tests.lc_image",
        "peekOfCode": "def load_image(image_path: str):\n    pil_image = Image.open(image_path)\n    image_b64 = convert_to_base64(pil_image)\n    print(\"Loaded image successfully!\")\n    return image_b64\nllm = Ollama(base_url=\"http://localhost:11434\", model=\"llava:latest\")\nimage_b64 = load_image(\"./images/chevy.jpg\")\nresp = llm.invoke(\"What's in the image?\", images=[image_b64])\nprint(resp)",
        "detail": "tests.lc_image",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "tests.lc_image",
        "description": "tests.lc_image",
        "peekOfCode": "llm = Ollama(base_url=\"http://localhost:11434\", model=\"llava:latest\")\nimage_b64 = load_image(\"./images/chevy.jpg\")\nresp = llm.invoke(\"What's in the image?\", images=[image_b64])\nprint(resp)",
        "detail": "tests.lc_image",
        "documentation": {}
    },
    {
        "label": "image_b64",
        "kind": 5,
        "importPath": "tests.lc_image",
        "description": "tests.lc_image",
        "peekOfCode": "image_b64 = load_image(\"./images/chevy.jpg\")\nresp = llm.invoke(\"What's in the image?\", images=[image_b64])\nprint(resp)",
        "detail": "tests.lc_image",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "tests.lc_image",
        "description": "tests.lc_image",
        "peekOfCode": "resp = llm.invoke(\"What's in the image?\", images=[image_b64])\nprint(resp)",
        "detail": "tests.lc_image",
        "documentation": {}
    },
    {
        "label": "get_image",
        "kind": 2,
        "importPath": "tests.llava",
        "description": "tests.llava",
        "peekOfCode": "def get_image(image_path):\n    with open(image_path, 'rb') as file:\n        return file.read()\ndef encode_image(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef generate_text(instruction, file_path):\n    result = ollama.generate(\n        model='minicpm-v:8b-2.6-q3_K_S',",
        "detail": "tests.llava",
        "documentation": {}
    },
    {
        "label": "encode_image",
        "kind": 2,
        "importPath": "tests.llava",
        "description": "tests.llava",
        "peekOfCode": "def encode_image(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef generate_text(instruction, file_path):\n    result = ollama.generate(\n        model='minicpm-v:8b-2.6-q3_K_S',\n        prompt=instruction,\n        images=[file_path],\n        stream=False",
        "detail": "tests.llava",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "tests.llava",
        "description": "tests.llava",
        "peekOfCode": "def generate_text(instruction, file_path):\n    result = ollama.generate(\n        model='minicpm-v:8b-2.6-q3_K_S',\n        prompt=instruction,\n        images=[file_path],\n        stream=False\n    )['response']\n    img=Image.open(file_path, mode='r')\n    img = img.resize([int(i/1.2) for i in img.size])\n    print(img) ",
        "detail": "tests.llava",
        "documentation": {}
    },
    {
        "label": "is_gpu_available",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def is_gpu_available():\n    if not GPU_AVAILABLE:\n        logging.warning(\"GPU support not available: nvgpu module not found\")\n        return {\"gpu_found\": False, \"num_gpus\": 0, \"first_gpu_vram\": 0, \"total_vram\": 0, \"error\": \"nvgpu module not found\"}\n    try:\n        gpu_info = nvgpu.gpu_info()\n        num_gpus = len(gpu_info)\n        if num_gpus == 0:\n            logging.warning(\"No GPUs found on the system\")\n            return {\"gpu_found\": False, \"num_gpus\": 0, \"first_gpu_vram\": 0, \"total_vram\": 0}",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def load_model(llm_model_name: str, raise_exception: bool = True):\n    global USE_VERBOSE\n    try:\n        current_file_path = os.path.abspath(__file__)\n        base_dir = os.path.dirname(current_file_path)\n        models_dir = os.path.join(base_dir, 'models')\n        matching_files = glob.glob(os.path.join(models_dir, f\"{llm_model_name}*\"))\n        if not matching_files:\n            logging.error(f\"Error: No model file found matching: {llm_model_name}\")\n            raise FileNotFoundError",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def get_tokenizer(model_name: str):\n    if model_name.lower().startswith(\"gpt-\"):\n        return tiktoken.encoding_for_model(model_name)\n    elif model_name.lower().startswith(\"claude-\"):\n        return AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", clean_up_tokenization_spaces=False)\n    elif model_name.lower().startswith(\"llama-\"):\n        return AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\", clean_up_tokenization_spaces=False)\n    else:\n        raise ValueError(f\"Unsupported model: {model_name}\")\ndef estimate_tokens(text: str, model_name: str) -> int:",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "estimate_tokens",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def estimate_tokens(text: str, model_name: str) -> int:\n    try:\n        tokenizer = get_tokenizer(model_name)\n        return len(tokenizer.encode(text))\n    except Exception as e:\n        logging.warning(f\"Error using tokenizer for {model_name}: {e}. Falling back to approximation.\")\n        return approximate_tokens(text)\ndef approximate_tokens(text: str) -> int:\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text.strip())",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "approximate_tokens",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def approximate_tokens(text: str) -> int:\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text.strip())\n    # Split on whitespace and punctuation, keeping punctuation\n    tokens = re.findall(r'\\b\\w+\\b|\\S', text)\n    count = 0\n    for token in tokens:\n        if token.isdigit():\n            count += max(1, len(token) // 2)  # Numbers often tokenize to multiple tokens\n        elif re.match(r'^[A-Z]{2,}$', token):  # Acronyms",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "chunk_text",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def chunk_text(text: str, max_chunk_tokens: int, model_name: str) -> List[str]:\n    chunks = []\n    tokenizer = get_tokenizer(model_name)\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    current_chunk = []\n    current_chunk_tokens = 0\n    for sentence in sentences:\n        sentence_tokens = len(tokenizer.encode(sentence))\n        if current_chunk_tokens + sentence_tokens > max_chunk_tokens:\n            chunks.append(' '.join(current_chunk))",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "split_long_sentence",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def split_long_sentence(sentence: str, max_tokens: int, model_name: str) -> List[str]:\n    words = sentence.split()\n    chunks = []\n    current_chunk = []\n    current_chunk_tokens = 0\n    tokenizer = get_tokenizer(model_name)\n    for word in words:\n        word_tokens = len(tokenizer.encode(word))\n        if current_chunk_tokens + word_tokens > max_tokens and current_chunk:\n            chunks.append(' '.join(current_chunk))",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "adjust_overlaps",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def adjust_overlaps(chunks: List[str], tokenizer, max_chunk_tokens: int, overlap_size: int = 50) -> List[str]:\n    adjusted_chunks = []\n    for i in range(len(chunks)):\n        if i == 0:\n            adjusted_chunks.append(chunks[i])\n        else:\n            overlap_tokens = len(tokenizer.encode(' '.join(chunks[i-1].split()[-overlap_size:])))\n            current_tokens = len(tokenizer.encode(chunks[i]))\n            if overlap_tokens + current_tokens > max_chunk_tokens:\n                overlap_adjusted = chunks[i].split()[:-overlap_size]",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n    gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n    kernel = np.ones((1, 1), np.uint8)\n    gray = cv2.dilate(gray, kernel, iterations=1)\n    return Image.fromarray(gray)\ndef convert_pdf_to_images(input_pdf_file_path: str, max_pages: int = 0, skip_first_n_pages: int = 0) -> List[Image.Image]:\n    logging.info(f\"Processing PDF file {input_pdf_file_path}\")\n    if max_pages == 0:\n        last_page = None",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "convert_pdf_to_images",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def convert_pdf_to_images(input_pdf_file_path: str, max_pages: int = 0, skip_first_n_pages: int = 0) -> List[Image.Image]:\n    logging.info(f\"Processing PDF file {input_pdf_file_path}\")\n    if max_pages == 0:\n        last_page = None\n        logging.info(\"Converting all pages to images...\")\n    else:\n        last_page = skip_first_n_pages + max_pages\n        logging.info(f\"Converting pages {skip_first_n_pages + 1} to {last_page}\")\n    first_page = skip_first_n_pages + 1  # pdf2image uses 1-based indexing\n    images = convert_from_path(input_pdf_file_path, first_page=first_page, last_page=last_page)",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "ocr_image",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def ocr_image(image):\n    preprocessed_image = preprocess_image(image)\n    return pytesseract.image_to_string(preprocessed_image)\nasync def process_chunk(chunk: str, prev_context: str, chunk_index: int, total_chunks: int, reformat_as_markdown: bool, suppress_headers_and_page_numbers: bool) -> Tuple[str, str]:\n    logging.info(f\"Processing chunk {chunk_index + 1}/{total_chunks} (length: {len(chunk):,} characters)\")\n    # Step 1: OCR Correction\n    ocr_correction_prompt = f\"\"\"Correct OCR-induced errors in the text, ensuring it flows coherently with the previous context. Follow these guidelines:\n1. Fix OCR-induced typos and errors:\n   - Correct words split across line breaks\n   - Fix common OCR errors (e.g., 'rn' misread as 'm')",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "remove_corrected_text_header",
        "kind": 2,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "def remove_corrected_text_header(text):\n    return text.replace(\"# Corrected text\\n\", \"\").replace(\"# Corrected text:\", \"\").replace(\"\\nCorrected text\", \"\").replace(\"Corrected text:\", \"\")\nasync def assess_output_quality(original_text, processed_text):\n    max_chars = 15000  # Limit to avoid exceeding token limits\n    available_chars_per_text = max_chars // 2  # Split equally between original and processed\n    original_sample = original_text[:available_chars_per_text]\n    processed_sample = processed_text[:available_chars_per_text]\n    prompt = f\"\"\"Compare the following samples of original OCR text with the processed output and assess the quality of the processing. Consider the following factors:\n1. Accuracy of error correction\n2. Improvement in readability",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "config = DecoupleConfig(RepositoryEnv('.env'))\nUSE_LOCAL_LLM = config.get(\"USE_LOCAL_LLM\", default=False, cast=bool)\nAPI_PROVIDER = config.get(\"API_PROVIDER\", default=\"OPENAI\", cast=str) # OPENAI or CLAUDE\nANTHROPIC_API_KEY = config.get(\"ANTHROPIC_API_KEY\", default=\"your-anthropic-api-key\", cast=str)\nOPENAI_API_KEY = config.get(\"OPENAI_API_KEY\", default=\"your-openai-api-key\", cast=str)\nCLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "USE_LOCAL_LLM",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "USE_LOCAL_LLM = config.get(\"USE_LOCAL_LLM\", default=False, cast=bool)\nAPI_PROVIDER = config.get(\"API_PROVIDER\", default=\"OPENAI\", cast=str) # OPENAI or CLAUDE\nANTHROPIC_API_KEY = config.get(\"ANTHROPIC_API_KEY\", default=\"your-anthropic-api-key\", cast=str)\nOPENAI_API_KEY = config.get(\"OPENAI_API_KEY\", default=\"your-openai-api-key\", cast=str)\nCLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "API_PROVIDER",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "API_PROVIDER = config.get(\"API_PROVIDER\", default=\"OPENAI\", cast=str) # OPENAI or CLAUDE\nANTHROPIC_API_KEY = config.get(\"ANTHROPIC_API_KEY\", default=\"your-anthropic-api-key\", cast=str)\nOPENAI_API_KEY = config.get(\"OPENAI_API_KEY\", default=\"your-openai-api-key\", cast=str)\nCLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "ANTHROPIC_API_KEY",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "ANTHROPIC_API_KEY = config.get(\"ANTHROPIC_API_KEY\", default=\"your-anthropic-api-key\", cast=str)\nOPENAI_API_KEY = config.get(\"OPENAI_API_KEY\", default=\"your-openai-api-key\", cast=str)\nCLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "OPENAI_API_KEY",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\", default=\"your-openai-api-key\", cast=str)\nCLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "CLAUDE_MODEL_STRING",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "CLAUDE_MODEL_STRING = config.get(\"CLAUDE_MODEL_STRING\", default=\"claude-3-haiku-20240307\", cast=str)\nCLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "CLAUDE_MAX_TOKENS",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "CLAUDE_MAX_TOKENS = 4096 # Maximum allowed tokens for Claude API\nTOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "TOKEN_BUFFER",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "TOKEN_BUFFER = 500  # Buffer to account for token estimation inaccuracies\nTOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "TOKEN_CUSHION",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "TOKEN_CUSHION = 300 # Don't use the full max tokens to avoid hitting the limit\nOPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "OPENAI_COMPLETION_MODEL",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "OPENAI_COMPLETION_MODEL = config.get(\"OPENAI_COMPLETION_MODEL\", default=\"gpt-4o-mini\", cast=str)\nOPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "OPENAI_EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "OPENAI_EMBEDDING_MODEL = config.get(\"OPENAI_EMBEDDING_MODEL\", default=\"text-embedding-3-small\", cast=str)\nOPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "OPENAI_MAX_TOKENS",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "OPENAI_MAX_TOKENS = 12000  # Maximum allowed tokens for OpenAI API\nDEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():\n    if not GPU_AVAILABLE:",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LOCAL_MODEL_NAME",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "DEFAULT_LOCAL_MODEL_NAME = \"Llama-3.1-8B-Lexi-Uncensored_Q5_fixedrope.gguf\"\nLOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():\n    if not GPU_AVAILABLE:\n        logging.warning(\"GPU support not available: nvgpu module not found\")",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "LOCAL_LLM_CONTEXT_SIZE_IN_TOKENS",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "LOCAL_LLM_CONTEXT_SIZE_IN_TOKENS = 2048\nUSE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():\n    if not GPU_AVAILABLE:\n        logging.warning(\"GPU support not available: nvgpu module not found\")\n        return {\"gpu_found\": False, \"num_gpus\": 0, \"first_gpu_vram\": 0, \"total_vram\": 0, \"error\": \"nvgpu module not found\"}",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "USE_VERBOSE",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "USE_VERBOSE = False\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():\n    if not GPU_AVAILABLE:\n        logging.warning(\"GPU support not available: nvgpu module not found\")\n        return {\"gpu_found\": False, \"num_gpus\": 0, \"first_gpu_vram\": 0, \"total_vram\": 0, \"error\": \"nvgpu module not found\"}\n    try:",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "openai_client",
        "kind": 5,
        "importPath": "tests.llm_aided_ocr",
        "description": "tests.llm_aided_ocr",
        "peekOfCode": "openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# GPU Check\ndef is_gpu_available():\n    if not GPU_AVAILABLE:\n        logging.warning(\"GPU support not available: nvgpu module not found\")\n        return {\"gpu_found\": False, \"num_gpus\": 0, \"first_gpu_vram\": 0, \"total_vram\": 0, \"error\": \"nvgpu module not found\"}\n    try:\n        gpu_info = nvgpu.gpu_info()",
        "detail": "tests.llm_aided_ocr",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.mock",
        "description": "tests.mock",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\nembeddings    = OllamaEmbeddings(model=EMBD)\nllm           = ChatOllama(\n    model=MODEL,\n    max_tokens=256,\n    keep_alive='1h',\n    temperature=0.0,\n)\nparser        = StrOutputParser()\nchain         = llm | parser",
        "detail": "tests.mock",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "kind": 6,
        "importPath": "tests.rag_service",
        "description": "tests.rag_service",
        "peekOfCode": "class RAGService():\n    \"\"\" Classe responsvel por converter arquivos PDF em Imagens.\n        Transforma as mesmas em textos, e inclui em base de dados (memria).\n    \"\"\"\n    def __init__(\n        self,\n        embedding_function: OllamaEmbeddings,\n        text_splitter: CharacterTextSplitter,\n        chain: Union[ChatOllama | StrOutputParser | ChatPromptTemplate],\n        chain_qr: Union[ChatOllama | StrOutputParser | ChatPromptTemplate] = None,",
        "detail": "tests.rag_service",
        "documentation": {}
    },
    {
        "label": "unicode_to_ascii",
        "kind": 2,
        "importPath": "tests.rag_service",
        "description": "tests.rag_service",
        "peekOfCode": "def unicode_to_ascii(s: str) -> str:\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\ndef normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n    return s.strip()",
        "detail": "tests.rag_service",
        "documentation": {}
    },
    {
        "label": "normalize_string",
        "kind": 2,
        "importPath": "tests.rag_service",
        "description": "tests.rag_service",
        "peekOfCode": "def normalize_string(s: str) -> str:\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n    return s.strip()\nclass RAGService():\n    \"\"\" Classe responsvel por converter arquivos PDF em Imagens.\n        Transforma as mesmas em textos, e inclui em base de dados (memria).\n    \"\"\"\n    def __init__(",
        "detail": "tests.rag_service",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.rag_tiny",
        "description": "tests.rag_tiny",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)\nembeddings    = OllamaEmbeddings(model=EMBD)\nllm           = ChatOllama(\n    model       = MODEL,\n    temperature = 0.0,\n    max_length  = 1000,\n    top_k       = 10,\n    keep_alive  = '1h'\n)\nparser        = StrOutputParser()",
        "detail": "tests.rag_tiny",
        "documentation": {}
    },
    {
        "label": "obter_contexto_relevante",
        "kind": 2,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "def obter_contexto_relevante(question: str, top_k: int = 2) -> List[Document]:\n    relevant_context = db.similarity_search_with_score(question, top_k)\n    return relevant_context\ndef ollama_chat(question: str, documentos_relevantes: List[Document], system_message: str, conversation_history: List) -> str:\n    conversation_history.append({\"role\": \"user\", \"content\": question})\n    contexto_relevante = []\n    for doc in documentos_relevantes:\n        contexto_relevante.append(doc[0].page_content)\n    contexto_relevante = '\\n'.join(contexto_relevante)\n    user_input_with_context = question",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "ollama_chat",
        "kind": 2,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "def ollama_chat(question: str, documentos_relevantes: List[Document], system_message: str, conversation_history: List) -> str:\n    conversation_history.append({\"role\": \"user\", \"content\": question})\n    contexto_relevante = []\n    for doc in documentos_relevantes:\n        contexto_relevante.append(doc[0].page_content)\n    contexto_relevante = '\\n'.join(contexto_relevante)\n    user_input_with_context = question\n    if contexto_relevante:\n        user_input_with_context = question + \"\\n\\Context:\\n\" + contexto_relevante\n    conversation_history[-1][\"content\"] = user_input_with_context",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "get_file_contents",
        "kind": 2,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "def get_file_contents(filepath: str) -> str:\n    with open(filepath, 'r', encoding='utf-8') as infile:\n        return infile.read()\nif __name__ == '__main__':\n    document_chunks = text_splitter.split_text(get_file_contents(\"cci.txt\"))\n    db              = DocArrayInMemorySearch.from_texts(document_chunks, embeddings)\n    print(\"Iniciando o loop de conversao...\")\n    conversation_history = []\n    system_message = \"You are a helpful assistant that is an expert at extracting information from a given text. Use only the context provided to craft a clear and detailed answer to the given question. Use language detection to ensure you respond in the same language as the user's question. If you don't know the answer, state that you don't know and do not provide unrelated information.\"\n    while True:",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "PINK",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "PINK = '\\033[95m'\nCYAN = '\\033[96m'\nYELLOW = '\\033[93m'\nNEON_GREEN = '\\033[92m'\nRESET_COLOR = '\\033[0m'\nembeddings    = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\noutput_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "CYAN",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "CYAN = '\\033[96m'\nYELLOW = '\\033[93m'\nNEON_GREEN = '\\033[92m'\nRESET_COLOR = '\\033[0m'\nembeddings    = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\noutput_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "YELLOW",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "YELLOW = '\\033[93m'\nNEON_GREEN = '\\033[92m'\nRESET_COLOR = '\\033[0m'\nembeddings    = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\noutput_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,\n    top_k=10,",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "NEON_GREEN",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "NEON_GREEN = '\\033[92m'\nRESET_COLOR = '\\033[0m'\nembeddings    = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\noutput_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,\n    top_k=10,\n    keep_alive='1h'",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "RESET_COLOR",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "RESET_COLOR = '\\033[0m'\nembeddings    = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\noutput_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,\n    top_k=10,\n    keep_alive='1h'\n)",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "output_parser = StrOutputParser()\nllm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,\n    top_k=10,\n    keep_alive='1h'\n)\nretriver      = llm | output_parser\ntext_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "llm = ChatOllama(\n    model=\"llama3.1:8b-instruct-q2_K\",\n    temperature=0.0,\n    max_length=250,\n    top_k=10,\n    keep_alive='1h'\n)\nretriver      = llm | output_parser\ntext_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)\ndb            = None",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.requery",
        "description": "tests.requery",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)\ndb            = None\ndef obter_contexto_relevante(question: str, top_k: int = 2) -> List[Document]:\n    relevant_context = db.similarity_search_with_score(question, top_k)\n    return relevant_context\ndef ollama_chat(question: str, documentos_relevantes: List[Document], system_message: str, conversation_history: List) -> str:\n    conversation_history.append({\"role\": \"user\", \"content\": question})\n    contexto_relevante = []\n    for doc in documentos_relevantes:\n        contexto_relevante.append(doc[0].page_content)",
        "detail": "tests.requery",
        "documentation": {}
    },
    {
        "label": "split_document",
        "kind": 2,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "def split_document(document, max_length=1000):\n    \"\"\"Split the document into sections of approximately max_length characters.\"\"\"\n    words = document.split()\n    sections = []\n    current_section = []\n    current_length = 0\n    for word in words:\n        if current_length + len(word) + 1 > max_length and current_section:\n            sections.append(' '.join(current_section))\n            current_section = []",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "summarize_section",
        "kind": 2,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "def summarize_section(section):\n    prompt = f\"Summarize the following text in a concise manner:\\n\\n{section}\"\n    return get_completion(prompt)\ndef answer_question(summaries, question):\n    context = \"\\n\\n\".join(summaries)\n    prompt = f\"Given the following context, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n    return get_completion(prompt)\ndef document_qa(document, questions):\n    # Step 1: Split the document\n    sections = split_document(document)",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "answer_question",
        "kind": 2,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "def answer_question(summaries, question):\n    context = \"\\n\\n\".join(summaries)\n    prompt = f\"Given the following context, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n    return get_completion(prompt)\ndef document_qa(document, questions):\n    # Step 1: Split the document\n    sections = split_document(document)\n    print(f\"Document split into {len(sections)} sections.\")\n    # Step 2: Summarize each section\n    summaries = []",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "document_qa",
        "kind": 2,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "def document_qa(document, questions):\n    # Step 1: Split the document\n    sections = split_document(document)\n    print(f\"Document split into {len(sections)} sections.\")\n    # Step 2: Summarize each section\n    summaries = []\n    for i, section in enumerate(sections):\n        summary = summarize_section(section)\n        summaries.append(summary)\n        print(f\"Section {i+1} summarized.\")",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "long_document",
        "kind": 5,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "long_document = \"\"\"\n[Insert a long document here. For brevity, we are using a placeholder. \nIn a real scenario, this would be a much longer text, maybe several \nparagraphs or pages about a specific topic.]\nThis is a long document about climate change. It discusses various aspects \nincluding causes, effects, and potential solutions. The document covers \ntopics such as greenhouse gas emissions, rising global temperatures, \nmelting ice caps, sea level rise, extreme weather events, impact on \nbiodiversity, and strategies for mitigation and adaptation.\nThe document also explores the economic implications of climate change, ",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "questions = [\n    \"What are the main causes of climate change mentioned in the document?\",\n    \"What are some of the effects of climate change discussed?\",\n    \"What solutions or strategies are proposed to address climate change?\"\n]\nresults = document_qa(long_document, questions)\nfor question, answer in results:\n    print(f\"\\nQ: {question}\")\n    print(f\"A: {answer}\")",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "tests.resumo_qa",
        "description": "tests.resumo_qa",
        "peekOfCode": "results = document_qa(long_document, questions)\nfor question, answer in results:\n    print(f\"\\nQ: {question}\")\n    print(f\"A: {answer}\")",
        "detail": "tests.resumo_qa",
        "documentation": {}
    },
    {
        "label": "draw_rectangle",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def draw_rectangle(result, img, index, color=(255, 100, 0)):\n    x = result['left'][index]\n    y = result['top'][index]\n    w = result['width'][index]\n    h = result['height'][index]\n    thickness = 2\n    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness)\n    return x, y, img\ndef write_text(text, x, y, img, text_size=12):\n    font_type = ImageFont.truetype('assets/font/calibri.ttf', text_size)",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "write_text",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def write_text(text, x, y, img, text_size=12):\n    font_type = ImageFont.truetype('assets/font/calibri.ttf', text_size)\n    img_pil   = Image.fromarray(img)\n    draw      = ImageDraw.Draw(img_pil)\n    draw.text((x, y - text_size), text, font=font_type, fill=(0, 0, 255))\n    img       = np.array(img_pil)\n    return img\ndef get_image_df(img_rgb):\n    config_pytesseract = f'--tessdata-dir assets/tessdata --oem 3 --psm 6' #-c preserve_interword_spaces=1 output-preserve-enabled=True'\n    return pytesseract.image_to_data(image=img_rgb, lang='por', config=config_pytesseract, output_type=Output.DATAFRAME)",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "get_image_df",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def get_image_df(img_rgb):\n    config_pytesseract = f'--tessdata-dir assets/tessdata --oem 3 --psm 6' #-c preserve_interword_spaces=1 output-preserve-enabled=True'\n    return pytesseract.image_to_data(image=img_rgb, lang='por', config=config_pytesseract, output_type=Output.DATAFRAME)\ndef get_image_data(img_rgb):\n    config_pytesseract = f'--tessdata-dir assets/tessdata --oem 3 --psm 6 '# -c preserve_interword_spaces=1 output-preserve-enabled=True'\n    return pytesseract.image_to_data(image=img_rgb, lang='por', config=config_pytesseract, output_type=Output.DICT)\ndef get_rgb_from_img(img_path):\n    image_bgr = cv2.imread(img_path)\n    img_rgb   = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    return img_rgb",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "get_image_data",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def get_image_data(img_rgb):\n    config_pytesseract = f'--tessdata-dir assets/tessdata --oem 3 --psm 6 '# -c preserve_interword_spaces=1 output-preserve-enabled=True'\n    return pytesseract.image_to_data(image=img_rgb, lang='por', config=config_pytesseract, output_type=Output.DICT)\ndef get_rgb_from_img(img_path):\n    image_bgr = cv2.imread(img_path)\n    img_rgb   = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    return img_rgb\ndef find_words_remove(img, img_data):\n    img_copy = img.copy()\n    words_remove = []",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "get_rgb_from_img",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def get_rgb_from_img(img_path):\n    image_bgr = cv2.imread(img_path)\n    img_rgb   = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    return img_rgb\ndef find_words_remove(img, img_data):\n    img_copy = img.copy()\n    words_remove = []\n    for i in range(0, len(img_data['text'])):\n        palavra = img_data['text'][i]\n        confidence = int(float(img_data['conf'][i]))",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "find_words_remove",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def find_words_remove(img, img_data):\n    img_copy = img.copy()\n    words_remove = []\n    for i in range(0, len(img_data['text'])):\n        palavra = img_data['text'][i]\n        confidence = int(float(img_data['conf'][i]))\n        threshold = 70\n        threshold2 = 65\n        if confidence < threshold2 and not palavra.isspace() and len(palavra):\n            words_remove.append(palavra)",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "linha_inteira",
        "kind": 2,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "def linha_inteira(df, original_image):\n    for line_num, words_per_line in df.groupby(\"line_num\"):\n        # filter out words with a low confidence\n        words_per_line = words_per_line[words_per_line[\"conf\"] >= 5]\n        if not len(words_per_line):\n            continue\n        words = words_per_line[\"text\"].values\n        line = \" \".join(words)\n        print(f\"{line_num} '{line}'\")\n        # if target_text in line:",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "pytesseract.pytesseract.tesseract_cmd",
        "kind": 5,
        "importPath": "tests.shape",
        "description": "tests.shape",
        "peekOfCode": "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"\ndef draw_rectangle(result, img, index, color=(255, 100, 0)):\n    x = result['left'][index]\n    y = result['top'][index]\n    w = result['width'][index]\n    h = result['height'][index]\n    thickness = 2\n    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness)\n    return x, y, img\ndef write_text(text, x, y, img, text_size=12):",
        "detail": "tests.shape",
        "documentation": {}
    },
    {
        "label": "PyTesseractParser",
        "kind": 6,
        "importPath": "tests.tessloader",
        "description": "tests.tessloader",
        "peekOfCode": "class PyTesseractParser(BaseBlobParser):\n    \"\"\"Carga un PDF con pdf2image y extrae texto usando pytesseract.\"\"\"\n    def __init__(self, pdf_path: str):\n        self._img_processing = ImageProcessing()\n        self._pdf_path       = pdf_path\n    def lazy_parse(self, blob: Union[Blob, None] = None) -> Iterator[Document]:\n        # recupera somente o nome do arquivo\n        __split       = self._pdf_path.split(\"/\")\n        # cria o nome base da imagem\n        __nome_imagem = __split[-1].replace('.pdf', '.png')",
        "detail": "tests.tessloader",
        "documentation": {}
    },
    {
        "label": "PyTesseractLoader",
        "kind": 6,
        "importPath": "tests.tessloader",
        "description": "tests.tessloader",
        "peekOfCode": "class PyTesseractLoader(BasePDFLoader):\n    def __init__(self, file_path: str) -> None:\n        self.parser = PyTesseractParser(pdf_path=file_path)\n        super().__init__(file_path)\n    def load(self) -> List[Document]:\n        return list(self.lazy_load())\n    def lazy_load(self) -> Iterator[Document]:\n        yield from self.parser.parse(None)\ndef main():\n    file_path = \"./files/pdfs/CCI1029.pdf\"",
        "detail": "tests.tessloader",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tests.tessloader",
        "description": "tests.tessloader",
        "peekOfCode": "def main():\n    file_path = \"./files/pdfs/CCI1029.pdf\"\n    loader    = PyTesseractLoader(file_path)\n    documents = loader.load()\n    for doc in documents:\n        print(f\"Pgina {doc.metadata['page']}:\")\n        print(doc.page_content)\nif __name__ == \"__main__\":\n    main()",
        "detail": "tests.tessloader",
        "documentation": {}
    },
    {
        "label": "FastAPIDeployment",
        "kind": 6,
        "importPath": "tests.teste",
        "description": "tests.teste",
        "peekOfCode": "class FastAPIDeployment:\n    # FastAPI will automatically parse the HTTP request for us.\n    @app.get(\"/hello\")\n    def say_hello(self, name: str) -> str:\n        return f\"Hello {name}!\"\n# 2: Deploy the deployment.\nserve.run(FastAPIDeployment.bind(), route_prefix=\"/\")\n# 3: Query the deployment and print the result.\nprint(requests.get(\"http://localhost:8000/hello\", params={\"name\": \"Theodore\"}).json())",
        "detail": "tests.teste",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "tests.teste",
        "description": "tests.teste",
        "peekOfCode": "app = FastAPI()\n@serve.deployment\n@serve.ingress(app)\nclass FastAPIDeployment:\n    # FastAPI will automatically parse the HTTP request for us.\n    @app.get(\"/hello\")\n    def say_hello(self, name: str) -> str:\n        return f\"Hello {name}!\"\n# 2: Deploy the deployment.\nserve.run(FastAPIDeployment.bind(), route_prefix=\"/\")",
        "detail": "tests.teste",
        "documentation": {}
    },
    {
        "label": "extrair_numeros_comunicados",
        "kind": 2,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "def extrair_numeros_comunicados(text: str) -> None:\n    result = re.findall('[CCI - ]+[ |0-9|.|-]{1,9}', text)\n    print (result)\nif __name__ == \"__main__\":\n    t1 = Thread(target=extrair_numeros_comunicados, args=(texto,))\n    t2 = Thread(target=extrair_numeros_comunicados, args=(texto2,))\n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "texto",
        "kind": 5,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "texto = \"\"\"\n## Comunicado: CCI - 1.081/2024 - CCS Braslia/DF, 8 de agosto de 2024.\nSICOOB Carta-Circular\nde Instruo/Funcionalidade &\nCCI - 1.081/2024 - CCS Braslia/DF, 8 de agosto de 2024.\ns entidades do Sicoob.\nCCI complementar, referente s evolues na funcionalidade Reavaliao Garantias do \nmdulo Conduo de Crdito da Plataforma de Crdito do Sisbr 2.0, em 9/8/2024.\nSenhores(as).\n1. Em complemento s CClIs - 007/2023, 129/2023, 522/2023, 1.015/2023 e 1.598/2023 -",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "texto2",
        "kind": 5,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "texto2 = \"\"\"\nCCI - 1.888/2020 - CCS Braslia/DF, 23 de novembro de 2023.\ns entidades do Sicoob.\nAssunto: Remanejamento de valores de benefcios do carto Coopcerto\n(alimentao e refeio) pela atendente digital Alice, por meio do \nWhatsApp.\nSenhores(as),\n1. Dando continuidade s melhorias e entregas direcionadas para as entidades CCI - 4321 do \nSicoob, informamos que, a partir de 1/12/2023, os empregados das cooperativas \ncentrais, das cooperativas singulares e do Centro Cooperativo Sicoob (CCS)",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "img",
        "kind": 5,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "img = cv2.imread('image.jpg')\n# Detect text regions\nrects = detector(img)\n# Extract text from regions\ntext = \"\"\nfor rect in rects:\n   x, y, w, h = rect\n   text += pytesseract.image_to_string(img[y:y+h, x:x+w])",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "rects",
        "kind": 5,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "rects = detector(img)\n# Extract text from regions\ntext = \"\"\nfor rect in rects:\n   x, y, w, h = rect\n   text += pytesseract.image_to_string(img[y:y+h, x:x+w])",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "tests.thread",
        "description": "tests.thread",
        "peekOfCode": "text = \"\"\nfor rect in rects:\n   x, y, w, h = rect\n   text += pytesseract.image_to_string(img[y:y+h, x:x+w])",
        "detail": "tests.thread",
        "documentation": {}
    },
    {
        "label": "get_flight_times",
        "kind": 2,
        "importPath": "tests.tool",
        "description": "tests.tool",
        "peekOfCode": "def get_flight_times(departure: str, arrival: str) -> str:\n  flights = {\n    'NYC-LAX': {'departure': '08:00 AM', 'arrival': '11:30 AM', 'duration': '5h 30m'},\n    'LAX-NYC': {'departure': '02:00 PM', 'arrival': '10:30 PM', 'duration': '5h 30m'},\n    'LHR-JFK': {'departure': '10:00 AM', 'arrival': '01:00 PM', 'duration': '8h 00m'},\n    'JFK-LHR': {'departure': '09:00 PM', 'arrival': '09:00 AM', 'duration': '7h 00m'},\n    'CDG-DXB': {'departure': '11:00 AM', 'arrival': '08:00 PM', 'duration': '6h 00m'},\n    'DXB-CDG': {'departure': '03:00 AM', 'arrival': '07:30 AM', 'duration': '7h 30m'},\n  }\n  key = f'{departure}-{arrival}'.upper()",
        "detail": "tests.tool",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "tests.tool",
        "description": "tests.tool",
        "peekOfCode": "prompt = PromptTemplate(\n            template=\"Voc  um assistente jurdico prestativo, respeitoso e honesto. Sua tarefa  auxiliar os advogados na anlise de aes e processos.\\n{format_instructions}\\n{question}\\n\\n\\n### Contexto ###\\n\\n```{context}```\",\n            input_variables=[\"question\"],\n            partial_variables={\"format_instructions\": format_instructions, \"context\": texto},\n        )\nresponse = ollama.chat(\n    model='kuqoi/qwen2-tools',\n    messages=[\n      {'role': 'system', 'content': \"Voc  um assistente jurdico prestativo, respeitoso e honesto. Sua tarefa  auxiliar os advogados na anlise de aes e processos.\\n{format_instructions}\\n{question}\\n\\n\\n### Contexto ###\\n\\n```{context}```\"}\n      {'role': 'user', 'content': 'Qual  o nmero do processo?'}",
        "detail": "tests.tool",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "tests.tool",
        "description": "tests.tool",
        "peekOfCode": "response = ollama.chat(\n    model='kuqoi/qwen2-tools',\n    messages=[\n      {'role': 'system', 'content': \"Voc  um assistente jurdico prestativo, respeitoso e honesto. Sua tarefa  auxiliar os advogados na anlise de aes e processos.\\n{format_instructions}\\n{question}\\n\\n\\n### Contexto ###\\n\\n```{context}```\"}\n      {'role': 'user', 'content': 'Qual  o nmero do processo?'}\n    ],\n\t\t# provide a weather checking tool to the model\n    tools=[{\n      'type': 'function',\n      'function': {",
        "detail": "tests.tool",
        "documentation": {}
    },
    {
        "label": "gerar_novas_queries_local",
        "kind": 2,
        "importPath": "tests.tst_queries",
        "description": "tests.tst_queries",
        "peekOfCode": "def gerar_novas_queries_local(query: str, contexto: str) -> Union[List[str], None]:\n    output = None\n    messages=[\n        {\"role\": \"system\", \"content\": \"Voc  um assistente dedicado a gerar novas perguntas a partir de uma pergunta base. Crie 1 nova pergunta sem perder o foco da pergunta original que foi passada na roler user.\"},\n        {\"role\": \"user\", \"content\": f\"{query}\"},\n    ]\n    # messages = [\n    #    (\"system\", \"Voc  um assistente dedicado a gerar novas perguntas a partir de uma pergunta base. Crie 1 nova pergunta sem perder o foco da pergunta original que foi passada na roler user.\"),\n    #    (\"user\", pergunta)\n    #]",
        "detail": "tests.tst_queries",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "tests.tst_queries",
        "description": "tests.tst_queries",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\nembeddings    = OllamaEmbeddings(model=EMBD)\nllm           = ChatOllama(\n    model=MODEL,\n    keep_alive='1h',\n    temperature=0.1,\n    top_k=10\n)\nllm_query     = ChatOllama(\n    model=MODEL_Q2,",
        "detail": "tests.tst_queries",
        "documentation": {}
    },
    {
        "label": "contexto_menor",
        "kind": 5,
        "importPath": "tests.tst_queries",
        "description": "tests.tst_queries",
        "peekOfCode": "contexto_menor = \"\"\"\n#### Introduo\nO Banco Sicoob tem como viso Ser reconhecido como a principal instituio financeira\npropulsora do desenvolvimento econmico e social dos associados das cooperativas do\nSicoob. A oferta dos produtos e servios de cmbio e comrcio exterior foi idealizada\ncom vistas a agregar competitividade e visibilidade ao portflio da instituio ante ao\nmercado.\nComo parte da soluo, o Banco Sicoob disponibilizar a troca de moeda estrangeira para\nfins de turismo. As transferncias de recursos at US$ 3.000,00 (trs mil dlares dos\nEstados Unidos) integram parte dessa soluo e sero disponibilizadas atravs de",
        "detail": "tests.tst_queries",
        "documentation": {}
    },
    {
        "label": "sess_options",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options = rt.SessionOptions()\nsess_options.enable_profiling = True\nsess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.enable_profiling",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.enable_profiling = True\nsess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.log_severity_level",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.log_severity_level = 0 # Verbose\nsess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())\n    await __loader.extract()",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.execution_mode",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.execution_mode = rt.ExecutionMode.ORT_PARALLEL\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())\n    await __loader.extract()\nasync def main():",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.graph_optimization_level",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())\n    await __loader.extract()\nasync def main():\n    t0 = time.time()",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.inter_op_num_threads",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.inter_op_num_threads = 4\nsess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())\n    await __loader.extract()\nasync def main():\n    t0 = time.time()\n    count = 0",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "sess_options.intra_op_num_threads",
        "kind": 5,
        "importPath": "carga_pdf",
        "description": "carga_pdf",
        "peekOfCode": "sess_options.intra_op_num_threads = 4\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\nasync def task(__folder: str, __arquivo: str):\n    __doc_path_pdf = __folder + __arquivo\n    __loader       = PdfExtractor(__doc_path_pdf, __arquivo, MetadataService())\n    await __loader.extract()\nasync def main():\n    t0 = time.time()\n    count = 0\n    __documents = []",
        "detail": "carga_pdf",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "eocr",
        "description": "eocr",
        "peekOfCode": "reader = easyocr.Reader(['pt','en'])\nresult = reader.readtext('./files/old_to_img/CCI1.301_2024_0_GRAY.jpeg', detail=0, paragraph=False, text_threshold=0.2)\nprint(result)",
        "detail": "eocr",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "eocr",
        "description": "eocr",
        "peekOfCode": "result = reader.readtext('./files/old_to_img/CCI1.301_2024_0_GRAY.jpeg', detail=0, paragraph=False, text_threshold=0.2)\nprint(result)",
        "detail": "eocr",
        "documentation": {}
    },
    {
        "label": "ChatApp",
        "kind": 6,
        "importPath": "gmain",
        "description": "gmain",
        "peekOfCode": "class ChatApp:\n    def __init__(self):\n        self.context_text = \"\"\n    def send_message(self, question):\n        payload = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"question\": {\n                    \"type\": \"string\",\n                    \"description\": question",
        "detail": "gmain",
        "documentation": {}
    },
    {
        "label": "get_html",
        "kind": 2,
        "importPath": "html_to_markdown",
        "description": "html_to_markdown",
        "peekOfCode": "def get_html(html_path: str) -> bytes:\n    with open(html_path, 'r', encoding='utf-8') as file:\n        return file.read()\ndef to_base64(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef main():\n    html_path = '/Users/rogerio.rodrigues/Documents/work_java_pessoal/tabbypdf-master/src/test/resources/pdf/edit/html/0702763-79.2024.8.07.0014_0001.pdf.0.html'\n    html = f\"\"\"<html><body>{get_html(html_path)}</body></html>\"\"\"",
        "detail": "html_to_markdown",
        "documentation": {}
    },
    {
        "label": "to_base64",
        "kind": 2,
        "importPath": "html_to_markdown",
        "description": "html_to_markdown",
        "peekOfCode": "def to_base64(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef main():\n    html_path = '/Users/rogerio.rodrigues/Documents/work_java_pessoal/tabbypdf-master/src/test/resources/pdf/edit/html/0702763-79.2024.8.07.0014_0001.pdf.0.html'\n    html = f\"\"\"<html><body>{get_html(html_path)}</body></html>\"\"\"\n    print(html)\n    response = ollama.chat(\n        model='reader-lm:1.5b-fp16',",
        "detail": "html_to_markdown",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "html_to_markdown",
        "description": "html_to_markdown",
        "peekOfCode": "def main():\n    html_path = '/Users/rogerio.rodrigues/Documents/work_java_pessoal/tabbypdf-master/src/test/resources/pdf/edit/html/0702763-79.2024.8.07.0014_0001.pdf.0.html'\n    html = f\"\"\"<html><body>{get_html(html_path)}</body></html>\"\"\"\n    print(html)\n    response = ollama.chat(\n        model='reader-lm:1.5b-fp16',\n        messages=[\n            {\n                'role': 'user',\n                'content': html",
        "detail": "html_to_markdown",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "leitor_tabelas",
        "description": "leitor_tabelas",
        "peekOfCode": "doc = fitz.open('./files/old_pdfs/CCI1.088_2024.pdf')\nfor page in doc:\n    tables = page.get_textbox()\n    for table in tables:\n        print(table)",
        "detail": "leitor_tabelas",
        "documentation": {}
    },
    {
        "label": "CONFIG_EMBDBERT",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "CONFIG_EMBDBERT = 'paraphrase-multilingual'\nCONFIG_EMBD  = 'mxbai-embed-large'\nMODEL_MISTRAL = 'mistral:7b-instruct-v0.3-q2_K'\nMODEL_LLAMA  = 'qwen2:1.5b-instruct-q4_K_M'\nMODEL_GEMMA  = 'gemma2:2b-instruct-q4_K_M'\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "MODEL_MISTRAL",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "MODEL_MISTRAL = 'mistral:7b-instruct-v0.3-q2_K'\nMODEL_LLAMA  = 'qwen2:1.5b-instruct-q4_K_M'\nMODEL_GEMMA  = 'gemma2:2b-instruct-q4_K_M'\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "faz_carga",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "faz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\nif __name__ == \"__main__\":\n    client = weaviate.WeaviateClient(\n        embedded_options=EmbeddedOptions(",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "env_path",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "env_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\nif __name__ == \"__main__\":\n    client = weaviate.WeaviateClient(\n        embedded_options=EmbeddedOptions(\n            additional_env_vars={",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "db_path",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "db_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\nif __name__ == \"__main__\":\n    client = weaviate.WeaviateClient(\n        embedded_options=EmbeddedOptions(\n            additional_env_vars={\n                \"ENABLE_MODULES\": \"backup-filesystem,text2vec-ollama,generative-ollama\",",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "weav_url",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "weav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\nif __name__ == \"__main__\":\n    client = weaviate.WeaviateClient(\n        embedded_options=EmbeddedOptions(\n            additional_env_vars={\n                \"ENABLE_MODULES\": \"backup-filesystem,text2vec-ollama,generative-ollama\",\n                \"BACKUP_FILESYSTEM_PATH\": \"/tmp/backups\",\n                \"PERSISTENCE_DATA_PATH\": db_path,\n                \"LIMIT_RESOURCES\": 'true'",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "weav_api_key",
        "kind": 5,
        "importPath": "start_weaviate",
        "description": "start_weaviate",
        "peekOfCode": "weav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\nif __name__ == \"__main__\":\n    client = weaviate.WeaviateClient(\n        embedded_options=EmbeddedOptions(\n            additional_env_vars={\n                \"ENABLE_MODULES\": \"backup-filesystem,text2vec-ollama,generative-ollama\",\n                \"BACKUP_FILESYSTEM_PATH\": \"/tmp/backups\",\n                \"PERSISTENCE_DATA_PATH\": db_path,\n                \"LIMIT_RESOURCES\": 'true'\n            }",
        "detail": "start_weaviate",
        "documentation": {}
    },
    {
        "label": "sumarizar_processos",
        "kind": 2,
        "importPath": "sucesso_extracao",
        "description": "sucesso_extracao",
        "peekOfCode": "def sumarizar_processos(pagina: str):\n    try:\n        summary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valor da dvida, requerentes, requeridos, as partes e objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva suas respostas no formato markdown.\"\n        payload = {\n            \"messages\": [\n                {\n                    \"content\": summary_juridico_prompt,\n                    \"role\": \"system\"\n                },\n                {",
        "detail": "sucesso_extracao",
        "documentation": {}
    },
    {
        "label": "PROMPT_NUM_PROCESSO",
        "kind": 5,
        "importPath": "sucesso_extracao",
        "description": "sucesso_extracao",
        "peekOfCode": "PROMPT_NUM_PROCESSO = \"\"\"\nUtilizando APEANAS o contexto. Encontre e escreva as informaes abaixo:\nO nmero do processo.\nA data da distribuio ou ltima distribuio.\nO valor da causa do processo.\nNome da parte autora da ao ou processo.\nNome do advogado da parte autora da ao ou processo.\nrgo julgador do processo.\nJuiz, ou qual o nome do Juiz de Direito citado no processo.\nEscreva sua resposta no formato",
        "detail": "sucesso_extracao",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "sucesso_extracao",
        "description": "sucesso_extracao",
        "peekOfCode": "headers = {\n    \"Content-Type\": \"application/json\"\n}\njan_ai_url = \"http://localhost:1337\"\ndef sumarizar_processos(pagina: str):\n    try:\n        summary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valor da dvida, requerentes, requeridos, as partes e objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva suas respostas no formato markdown.\"\n        payload = {\n            \"messages\": [\n                {",
        "detail": "sucesso_extracao",
        "documentation": {}
    },
    {
        "label": "jan_ai_url",
        "kind": 5,
        "importPath": "sucesso_extracao",
        "description": "sucesso_extracao",
        "peekOfCode": "jan_ai_url = \"http://localhost:1337\"\ndef sumarizar_processos(pagina: str):\n    try:\n        summary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valor da dvida, requerentes, requeridos, as partes e objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva suas respostas no formato markdown.\"\n        payload = {\n            \"messages\": [\n                {\n                    \"content\": summary_juridico_prompt,\n                    \"role\": \"system\"\n                },",
        "detail": "sucesso_extracao",
        "documentation": {}
    },
    {
        "label": "get_image",
        "kind": 2,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "def get_image(image_path) -> bytes:\n    with open(image_path, 'rb') as file:\n        return file.read()\ndef to_base64(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef main():\n    img_path_00 = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/files/old_to_img/0702763-79.2024.8.07.0014_0010_0_GRAY.jpeg'\n    response = ollama.chat(",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "to_base64",
        "kind": 2,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "def to_base64(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef main():\n    img_path_00 = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/files/old_to_img/0702763-79.2024.8.07.0014_0010_0_GRAY.jpeg'\n    response = ollama.chat(\n        model='minicpm-v:8b-2.6-q5_K_M',\n        messages=[\n            {",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "def main():\n    img_path_00 = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/files/old_to_img/0702763-79.2024.8.07.0014_0010_0_GRAY.jpeg'\n    response = ollama.chat(\n        model='minicpm-v:8b-2.6-q5_K_M',\n        messages=[\n            {\n                'role': 'user',\n                'content': f\"{PROMPT_NUM_PROCESSO}\",\n                'images': [to_base64(img_path_00)]\n            },",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_NUM_PROCESSO",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_NUM_PROCESSO = \"\"\"\nAplique OCR no texto da imagem. Encontre e escreva as informaes abaixo:\nO nmero do processo.\nA data da distribuio ou ltima distribuio.\nO valor da causa do processo.\nNome da parte autora da ao ou processo.\nNome do advogado da parte autora da ao ou processo.\nrgo julgador do processo.\nEscreva sua resposta no formato\n{",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_PARTES",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_PARTES = \"\"\"\nEncontre e escreva as informaes abaixo:\nNome e CPF/CNPJ de pessoas ou bancos citados no processo.\nEscreva sua resposta no formato\n{\n    \"pessoas_bancos\": [ \"nome_pessoa_banco\": {nome_pessoa_banco}, \"cpf_cnpj_pessoa_banco\": {cpf_cnpj_pessoa_banco} ]\n}\nPense passo a passo antes de escrever sua resposta.\n\"\"\"\nPROMPT_DATA_DISTRITUICAO = \"\"\"",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_DATA_DISTRITUICAO",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_DATA_DISTRITUICAO = \"\"\"\nEscreva a data da distribuio ou ltima distribuio descrita na imagem.\n\"\"\"\nPROMPT_CLASSE_TIPO_PROCESSO = \"\"\"\nEscreva o Tipo de Processo, ou Classe, ou Classe judicial descrito(a) na imagem.\n\"\"\"\nPROMPT_AUTOR = \"\"\"\nQuais so as partes (autor(a), advogado(s), ru(s)) citadas no processo?\nEscreva sua resposta no formato abaixo:\n```",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_CLASSE_TIPO_PROCESSO",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_CLASSE_TIPO_PROCESSO = \"\"\"\nEscreva o Tipo de Processo, ou Classe, ou Classe judicial descrito(a) na imagem.\n\"\"\"\nPROMPT_AUTOR = \"\"\"\nQuais so as partes (autor(a), advogado(s), ru(s)) citadas no processo?\nEscreva sua resposta no formato abaixo:\n```\nPartes: [<nome_parte>] // lista com os nomes das partes citadas\n```\n\"\"\"",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_AUTOR",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_AUTOR = \"\"\"\nQuais so as partes (autor(a), advogado(s), ru(s)) citadas no processo?\nEscreva sua resposta no formato abaixo:\n```\nPartes: [<nome_parte>] // lista com os nomes das partes citadas\n```\n\"\"\"\nPROMPT_CPF_CNPJ = \"\"\"\nEscreva os nomes e CPF/CNPJ das partes do processo descritos(as) nas imagens.\n\"\"\"",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "PROMPT_CPF_CNPJ",
        "kind": 5,
        "importPath": "sucesso_vision",
        "description": "sucesso_vision",
        "peekOfCode": "PROMPT_CPF_CNPJ = \"\"\"\nEscreva os nomes e CPF/CNPJ das partes do processo descritos(as) nas imagens.\n\"\"\"\ndef get_image(image_path) -> bytes:\n    with open(image_path, 'rb') as file:\n        return file.read()\ndef to_base64(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")",
        "detail": "sucesso_vision",
        "documentation": {}
    },
    {
        "label": "MaxResize",
        "kind": 6,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "class MaxResize(object):\n    def __init__(self, max_size=800):\n        self.max_size = max_size\n    def __call__(self, image):\n        width, height = image.size\n        current_max_size = max(width, height)\n        scale = self.max_size / current_max_size\n        resized_image = image.resize(\n            (int(round(scale * width)), int(round(scale * height)))\n        )",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "box_cxcywh_to_xyxy",
        "kind": 2,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "def box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\ndef rescale_bboxes(out_bbox, size):\n    width, height = size\n    boxes = box_cxcywh_to_xyxy(out_bbox)\n    boxes = boxes * torch.tensor(\n        [width-80, height-80, width+40, height+40], dtype=torch.float32\n    )",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "rescale_bboxes",
        "kind": 2,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "def rescale_bboxes(out_bbox, size):\n    width, height = size\n    boxes = box_cxcywh_to_xyxy(out_bbox)\n    boxes = boxes * torch.tensor(\n        [width-80, height-80, width+40, height+40], dtype=torch.float32\n    )\n    return boxes\ndef outputs_to_objects(outputs, img_size, id2label):\n    m = outputs.logits.softmax(-1).max(-1)\n    pred_labels = list(m.indices.detach().cpu().numpy())[0]",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "outputs_to_objects",
        "kind": 2,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "def outputs_to_objects(outputs, img_size, id2label):\n    m = outputs.logits.softmax(-1).max(-1)\n    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n    pred_scores = list(m.values.detach().cpu().numpy())[0]\n    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n    pred_bboxes = [\n        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)\n    ]\n    objects = []\n    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "detect_and_crop_save_table",
        "kind": 2,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "def detect_and_crop_save_table(\n    file_path, cropped_table_directory=\"./table_images/\"\n):\n    image = Image.open(file_path).convert(\"RGB\")\n    filename, _ = os.path.splitext(file_path.split(\"/\")[-1])\n    if not os.path.exists(cropped_table_directory):\n        os.makedirs(cropped_table_directory)\n    # prepare image for the model\n    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n    # forward pass",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "plot_images",
        "kind": 2,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "def plot_images(image_paths):\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n            plt.subplot(2, 3, images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclass MaxResize(object):\n    def __init__(self, max_size=800):\n        self.max_size = max_size\n    def __call__(self, image):\n        width, height = image.size\n        current_max_size = max(width, height)\n        scale = self.max_size / current_max_size\n        resized_image = image.resize(\n            (int(round(scale * width)), int(round(scale * height)))",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "detection_transform",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "detection_transform = transforms.Compose(\n    [\n        MaxResize(800),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)\nstructure_transform = transforms.Compose(\n    [\n        MaxResize(1000),",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "structure_transform",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "structure_transform = transforms.Compose(\n    [\n        MaxResize(1000),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)\n# load table detection model\n# processor = TableTransformerImageProcessor(max_size=800)\nmodel = AutoModelForObjectDetection.from_pretrained(",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "model = AutoModelForObjectDetection.from_pretrained(\n    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n).to(device)\n# load table structure recognition model\n# structure_processor = TableTransformerImageProcessor(max_size=1000)\nstructure_model = AutoModelForObjectDetection.from_pretrained(\n    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n).to(device)\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "structure_model",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "structure_model = AutoModelForObjectDetection.from_pretrained(\n    \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n).to(device)\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\ndef rescale_bboxes(out_bbox, size):\n    width, height = size",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "retrieved_images",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "retrieved_images = []\nimg_path_00 = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/files/old_to_img/CCI1.301_2024_1_GRAY.jpeg'\nretrieved_images.append(img_path_00)\nfor file_path in retrieved_images:\n    detect_and_crop_save_table(file_path)",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "img_path_00",
        "kind": 5,
        "importPath": "table_transformer",
        "description": "table_transformer",
        "peekOfCode": "img_path_00 = 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/files/old_to_img/CCI1.301_2024_1_GRAY.jpeg'\nretrieved_images.append(img_path_00)\nfor file_path in retrieved_images:\n    detect_and_crop_save_table(file_path)",
        "detail": "table_transformer",
        "documentation": {}
    },
    {
        "label": "get_ollama_embeddings_basic",
        "kind": 2,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "def get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    print(__embed)\n    return __embed\nasync def main():\n    __loader    = PdfExtractor('', '', MetadataService())\n    __documents = await __loader.extract_all()\n    client = weaviate.WeaviateClient(",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "CONFIG_EMBDBERT",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "CONFIG_EMBDBERT = 'paraphrase-multilingual'\nCONFIG_EMBD  = 'mxbai-embed-large'\nMODEL_MISTRAL = 'mistral:7b-instruct-v0.3-q2_K'\nMODEL_LLAMA  = 'qwen2:1.5b-instruct-q4_K_M'\nMODEL_GEMMA  = 'gemma2:2b-instruct-q4_K_M'\nVECTOR_NAME = \"sicoob_juridico_vector\"\nsummary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valores de dvidas, requerentes, requeridos, datas, as partes, comarca, juiz, fase do proesso, produto ou servios do Sicoob, CPF ou CNPJ das partes, o que deseja ou pede a parte autora e o objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva sua resposta em formato JSON.\\n\\n\\n{page_content}\"\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "MODEL_MISTRAL",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "MODEL_MISTRAL = 'mistral:7b-instruct-v0.3-q2_K'\nMODEL_LLAMA  = 'qwen2:1.5b-instruct-q4_K_M'\nMODEL_GEMMA  = 'gemma2:2b-instruct-q4_K_M'\nVECTOR_NAME = \"sicoob_juridico_vector\"\nsummary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valores de dvidas, requerentes, requeridos, datas, as partes, comarca, juiz, fase do proesso, produto ou servios do Sicoob, CPF ou CNPJ das partes, o que deseja ou pede a parte autora e o objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva sua resposta em formato JSON.\\n\\n\\n{page_content}\"\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "VECTOR_NAME",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "VECTOR_NAME = \"sicoob_juridico_vector\"\nsummary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valores de dvidas, requerentes, requeridos, datas, as partes, comarca, juiz, fase do proesso, produto ou servios do Sicoob, CPF ou CNPJ das partes, o que deseja ou pede a parte autora e o objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva sua resposta em formato JSON.\\n\\n\\n{page_content}\"\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "summary_juridico_prompt",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "summary_juridico_prompt = \"Voc  um assistente especialista em processos judiciais. Sua tarefa  fazer um resumo claro e conciso de processos, foque em aspectos como nmero do processo, valor da causa, valores de dvidas, requerentes, requeridos, datas, as partes, comarca, juiz, fase do proesso, produto ou servios do Sicoob, CPF ou CNPJ das partes, o que deseja ou pede a parte autora e o objetivo do processo. No acrescente nenhum conhecimento prvio, nota ou sugesto. Escreva sua resposta em formato JSON.\\n\\n\\n{page_content}\"\nfaz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "faz_carga",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "faz_carga = False\nenv_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "env_path",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "env_path = Path('/home/rogerio_rodrigues/python-workspace/rag_python/.env') # C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/.env')\ndb_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "db_path",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "db_path = '/home/rogerio_rodrigues/python-workspace/rag_python/db_weaviate/' # 'C:/Users/rogerio.rodrigues/Documents/workspace_python/rag_python/db_weaviate/'\nload_dotenv(dotenv_path=env_path)\n# Best practice: store your credentials in environment variables\nweav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    print(__embed)",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "weav_url",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "weav_url = os.getenv(\"WEAVIATE_URL\")\nweav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    print(__embed)\n    return __embed\nasync def main():\n    __loader    = PdfExtractor('', '', MetadataService())",
        "detail": "test_wvt",
        "documentation": {}
    },
    {
        "label": "weav_api_key",
        "kind": 5,
        "importPath": "test_wvt",
        "description": "test_wvt",
        "peekOfCode": "weav_api_key = os.getenv(\"WEAVIATE_ADMIN_KEY\")\ndef get_ollama_embeddings_basic() -> Union[OllamaEmbeddings, None]:\n    \"\"\" LLM para embeddings \"\"\"\n    print(f\"Criando o OllamaEmbeddings Basic\")\n    __embed = OllamaEmbeddings(model=CONFIG_EMBDBERT)\n    print(__embed)\n    return __embed\nasync def main():\n    __loader    = PdfExtractor('', '', MetadataService())\n    __documents = await __loader.extract_all()",
        "detail": "test_wvt",
        "documentation": {}
    }
]